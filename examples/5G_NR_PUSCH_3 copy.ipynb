{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d021b529",
   "metadata": {},
   "source": [
    "# 5G NR PUSCH Tutorial\n",
    "\n",
    "This notebook provides an introduction to Sionna's [5G New Radio (NR) module](https://nvlabs.github.io/sionna/api/nr.html) and, in particular, the [physical uplink shared channel (PUSCH)](https://nvlabs.github.io/sionna/api/nr.html#pusch). This module provides implementations of a small subset of the physical layer functionalities as described in the 3GPP specifications [38.211](https://portal.3gpp.org/desktopmodules/Specifications/SpecificationDetails.aspx?specificationId=3213), [38.212](https://portal.3gpp.org/desktopmodules/Specifications/SpecificationDetails.aspx?specificationId=3214) and [38.214](https://portal.3gpp.org/desktopmodules/Specifications/SpecificationDetails.aspx?specificationId=3216). \n",
    "\n",
    "\n",
    "You will\n",
    "\n",
    "- Get an understanding of the different components of a PUSCH configuration, such as the carrier, DMRS, and transport block,\n",
    "- Learn how to rapidly simulate PUSCH transmissions for multiple transmitters,\n",
    "- Modify the PUSCHReceiver to use a custom MIMO Detector.\n",
    "\n",
    "## Table of Contents\n",
    "* [GPU Configuration and Imports](#GPU-Configuration-and-Imports)\n",
    "* [A \"Hello World!\" Example](#A-Hello-World-Example)\n",
    "* [Carrier Configuration](#Carrier-Configuration)\n",
    "* [Understanding the DMRS Configuration](#Understanding-the-DMRS-Configuration)\n",
    "    * [Configuring Multiple Layers](#Configuring-Multiple-Layers)\n",
    "    * [Controlling the Number of DMRS Symbols in a Slot](#Controlling-the-Number-of-DMRS-Symbols-in-a-Slot)\n",
    "    * [How to control the number of available DMRS ports?](#How-to-control-the-number-of-available-DMRS-ports?)\n",
    "* [Transport Blocks and MCS](#Transport-Blocks-and-MCS)\n",
    "* [Looking into the PUSCHTransmitter](#Looking-into-the-PUSCHTransmitter)\n",
    "* [Components of the PUSCHReceiver](#Components-of-the-PUSCHReceiver)\n",
    "* [End-to-end PUSCH Simulations](#End-to-end-PUSCH-Simulations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3174011",
   "metadata": {
    "tags": []
   },
   "source": [
    "## GPU Configuration and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea40a820-e6b6-4fa6-9eed-0a2eea001498",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T15:42:26.278222Z",
     "iopub.status.busy": "2024-09-26T15:42:26.278038Z",
     "iopub.status.idle": "2024-09-26T15:42:28.860958Z",
     "shell.execute_reply": "2024-09-26T15:42:28.860378Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if os.getenv(\"CUDA_VISIBLE_DEVICES\") is None:\n",
    "    gpu_num = 0 # Use \"\" to use the CPU\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_num}\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "# tahtan\n",
    "# Import Sionna\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import sionna\n",
    "\n",
    "# try:\n",
    "#     import sionna\n",
    "# except ImportError as e:\n",
    "#     # Install Sionna if package is not already installed\n",
    "#     import os\n",
    "#     os.system(\"pip install sionna\")\n",
    "#     import sionna\n",
    "\n",
    "import tensorflow as tf\n",
    "# Configure the notebook to use only a single GPU and allocate only as much memory as needed\n",
    "# For more details, see https://www.tensorflow.org/guide/gpu\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "# Avoid warnings from TensorFlow\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "sionna.config.seed = 42 # Set seed for reproducible results\n",
    "\n",
    "# Load the required Sionna components\n",
    "from sionna.nr import PUSCHConfig, PUSCHTransmitter, PUSCHReceiver, CarrierConfig, PUSCHDMRSConfig, TBConfig, PUSCHPilotPattern, TBEncoder\n",
    "from sionna.nr.utils import generate_prng_seq\n",
    "from sionna.channel import AWGN, RayleighBlockFading, OFDMChannel, TimeChannel, time_lag_discrete_time_channel\n",
    "from sionna.channel.utils import * \n",
    "from sionna.channel.tr38901 import Antenna, AntennaArray, UMi, UMa, RMa, TDL, CDL\n",
    "from sionna.channel import gen_single_sector_topology as gen_topology\n",
    "from sionna.utils import compute_ber, ebnodb2no, sim_ber, array_to_hash, create_timestamped_folders, b2b, f2f, BinarySource\n",
    "from sionna.ofdm import KBestDetector, LinearDetector, MaximumLikelihoodDetector, ResourceGrid, ResourceGridMapper, OFDMModulator\n",
    "from sionna.mimo import StreamManagement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "376e6700",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T15:42:28.864569Z",
     "iopub.status.busy": "2024-09-26T15:42:28.864310Z",
     "iopub.status.idle": "2024-09-26T15:42:28.871785Z",
     "shell.execute_reply": "2024-09-26T15:42:28.871431Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "# from bs4 import BeautifulSoup\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cdcae8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## A Hello World Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cfc8d3",
   "metadata": {},
   "source": [
    "Let us start with a simple \"Hello, World!\" example in which we will simulate PUSCH transmissions from a single transmitter to a single receiver over an AWGN channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88ccdcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tx = 1\n",
    "num_rx = 1\n",
    "num_rx_ant = 8\n",
    "num_tx_ant = 1\n",
    "carrier_frequency = 2.55e-9\n",
    "\n",
    "# Configure antenna arrays\n",
    "ue_antenna = Antenna(polarization=\"single\",\n",
    "                polarization_type=\"V\",\n",
    "                antenna_pattern=\"38.901\",\n",
    "                carrier_frequency=carrier_frequency)\n",
    "\n",
    "gnb_array = AntennaArray(num_rows=1,\n",
    "                        num_cols=num_rx_ant//2,\n",
    "                        polarization=\"dual\",\n",
    "                        polarization_type=\"cross\",\n",
    "                        antenna_pattern=\"38.901\",\n",
    "                        carrier_frequency=carrier_frequency)\n",
    "\n",
    "channel_model_0 = CDL(\"C\", 150e-9, carrier_frequency, ue_antenna, gnb_array, \"uplink\", min_speed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12024222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carrier Configuration\n",
      "=====================\n",
      "cyclic_prefix : normal\n",
      "cyclic_prefix_length : 2.3437500000000002e-06\n",
      "frame_duration : 0.01\n",
      "frame_number : 0\n",
      "kappa : 64.0\n",
      "mu : 1\n",
      "n_cell_id : 0\n",
      "n_size_grid : 273\n",
      "n_start_grid : 0\n",
      "num_slots_per_frame : 20\n",
      "num_slots_per_subframe : 2\n",
      "num_symbols_per_slot : 14\n",
      "slot_number : 4\n",
      "sub_frame_duration : 0.001\n",
      "subcarrier_spacing : 30\n",
      "t_c : 5.086263020833334e-10\n",
      "t_s : 3.2552083333333335e-08\n",
      "\n",
      "PUSCH Configuration\n",
      "===================\n",
      "dmrs_grid : shape (1, 3276, 14)\n",
      "dmrs_grid_precoded : shape ()\n",
      "dmrs_mask : shape (3276, 14)\n",
      "dmrs_symbol_indices : [2, 11]\n",
      "frequency_hopping : neither\n",
      "l : [2, 11]\n",
      "l_0 : 2\n",
      "l_bar : [2, 11]\n",
      "l_d : 14\n",
      "l_prime : [0]\n",
      "l_ref : 0\n",
      "mapping_type : A\n",
      "n : shape (819,)\n",
      "n_rnti : 2008\n",
      "n_size_bwp : 273\n",
      "n_start_bwp : 0\n",
      "num_antenna_ports : 1\n",
      "num_coded_bits : 78624\n",
      "num_layers : 1\n",
      "num_ov : 0\n",
      "num_res_per_prb : 144\n",
      "num_resource_blocks : 273\n",
      "num_subcarriers : 3276\n",
      "precoding : non-codebook\n",
      "precoding_matrix : None\n",
      "symbol_allocation : [0, 14]\n",
      "tb_size : 52224\n",
      "tpmi : 0\n",
      "transform_precoding : False\n",
      "\n",
      "PUSCH DMRS Configuration\n",
      "========================\n",
      "additional_position : 1\n",
      "allowed_dmrs_ports : [0, 1, 2, 3]\n",
      "beta : 1.4142135623730951\n",
      "cdm_groups : [0]\n",
      "config_type : 1\n",
      "deltas : [0]\n",
      "dmrs_port_set : [0]\n",
      "length : 1\n",
      "n_id : [0, 0]\n",
      "n_scid : 0\n",
      "num_cdm_groups_without_data : 2\n",
      "type_a_position : 2\n",
      "w_f : [[1]\n",
      " [1]]\n",
      "w_t : [[1]\n",
      " [1]]\n",
      "\n",
      "Transport Block Configuration\n",
      "=============================\n",
      "channel_type : PUSCH\n",
      "mcs_index : 9\n",
      "mcs_table : 1\n",
      "n_id : 0\n",
      "num_bits_per_symbol : 2\n",
      "target_coderate : 0.6630859375\n",
      "tb_scaling : 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class MyPUSCHConfig(PUSCHConfig):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            carrier_config=CarrierConfig(\n",
    "                n_cell_id=0,\n",
    "                cyclic_prefix=\"normal\",\n",
    "                subcarrier_spacing=30,\n",
    "                n_size_grid=273,\n",
    "                n_start_grid=0,\n",
    "                slot_number=4,\n",
    "                frame_number=0\n",
    "            ),\n",
    "            pusch_dmrs_config=PUSCHDMRSConfig(\n",
    "                config_type=1,\n",
    "                length=1,\n",
    "                additional_position=1,\n",
    "                dmrs_port_set=[0],\n",
    "                n_id=0,\n",
    "                n_scid=0,\n",
    "                num_cdm_groups_without_data=2,\n",
    "                type_a_position=2\n",
    "            ),\n",
    "            tb_config=TBConfig(\n",
    "                channel_type='PUSCH',\n",
    "                n_id=0,\n",
    "                mcs_table=1,\n",
    "                mcs_index=9\n",
    "            ),\n",
    "            mapping_type='A',\n",
    "            n_size_bwp=273,\n",
    "            n_start_bwp=0,\n",
    "            num_layers=1,\n",
    "            num_antenna_ports=1,\n",
    "            precoding='non-codebook',\n",
    "            tpmi=0,\n",
    "            transform_precoding=False,\n",
    "            n_rnti=2008,\n",
    "            symbol_allocation=[0,14]\n",
    "        )\n",
    "\n",
    "pusch_config_0 = MyPUSCHConfig()\n",
    "pusch_config_0.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7702e174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b_0 = BinarySource()([1, 1, pusch_config_0.tb_size])\n",
    "# c_0 = BinarySource()([1, 1, pusch_config_0.num_coded_bits])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36613bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transmitter_0 = PUSCHTransmitter(pusch_config_0, return_bits=False)\n",
    "# x_0 = transmitter_0._mapper(c_0)\n",
    "# x_0 = transmitter_0._layer_mapper(x_0)\n",
    "# x_0 = transmitter_0._resource_grid_mapper(x_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41eef3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pusch_config_1 = pusch_config_0.clone()\n",
    "# pusch_config_1.carrier.frame_number = 77\n",
    "# transmitter_1 = PUSCHTransmitter(pusch_config_1, return_bits=False)\n",
    "# x_1 = transmitter_1._mapper(c_0)\n",
    "# x_1 = transmitter_1._layer_mapper(x_1)\n",
    "# x_1 = transmitter_1._resource_grid_mapper(x_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05fe7e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pusch_config_2 = pusch_config_0.clone()\n",
    "# pusch_config_2.carrier.slot_number = 14\n",
    "# transmitter_2 = PUSCHTransmitter(pusch_config_2, return_bits=False)\n",
    "# x_2 = transmitter_2._mapper(c_0)\n",
    "# x_2 = transmitter_2._layer_mapper(x_2)\n",
    "# x_2 = transmitter_2._resource_grid_mapper(x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "774a9497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pusch_config_3 = pusch_config_0.clone()\n",
    "# pusch_config_3.carrier.n_cell_id = 71\n",
    "# transmitter_3 = PUSCHTransmitter(pusch_config_3, return_bits=False)\n",
    "# x_3 = transmitter_3._mapper(c_0)\n",
    "# x_3 = transmitter_3._layer_mapper(x_3)\n",
    "# x_3 = transmitter_3._resource_grid_mapper(x_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44a41b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pusch_config_4 = pusch_config_0.clone()\n",
    "# pusch_config_4.dmrs.n_scid = 1\n",
    "# transmitter_4 = PUSCHTransmitter(pusch_config_4, return_bits=False)\n",
    "# x_4 = transmitter_4._mapper(c_0)\n",
    "# x_4= transmitter_4._layer_mapper(x_4)\n",
    "# x_4 = transmitter_4._resource_grid_mapper(x_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "517e5c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pusch_config_5 = pusch_config_0.clone()\n",
    "# pusch_config_5.dmrs.n_id = 13\n",
    "# transmitter_5 = PUSCHTransmitter(pusch_config_5, return_bits=False)\n",
    "# x_5 = transmitter_5._mapper(c_0)\n",
    "# x_5 = transmitter_5._layer_mapper(x_5)\n",
    "# x_5 = transmitter_5._resource_grid_mapper(x_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f94fa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pusch_config_6 = pusch_config_0.clone()\n",
    "# pusch_config_6.tb.n_id = 41\n",
    "# transmitter_6 = PUSCHTransmitter(pusch_config_6, return_bits=False)\n",
    "# x_6 = transmitter_6._mapper(c_0)\n",
    "# x_6 = transmitter_6._layer_mapper(x_6)\n",
    "# x_6 = transmitter_6._resource_grid_mapper(x_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3eae4b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pusch_config_7 = pusch_config_0.clone()\n",
    "# pusch_config_7.n_rnti = 31\n",
    "# transmitter_7 = PUSCHTransmitter(pusch_config_7, return_bits=False)\n",
    "# x_7 = transmitter_7._mapper(c_0)\n",
    "# x_7 = transmitter_7._layer_mapper(x_7)\n",
    "# x_7 = transmitter_7._resource_grid_mapper(x_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f679e985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.var(transmitter_0(b_0) - transmitter_1(b_0)), np.var(transmitter_0(b_0) - transmitter_2(b_0)),\\\n",
    "# np.var(transmitter_0(b_0) - transmitter_3(b_0)), np.var(transmitter_0(b_0) - transmitter_4(b_0)),\\\n",
    "# np.var(transmitter_0(b_0) - transmitter_5(b_0)), np.var(transmitter_0(b_0) - transmitter_6(b_0)),\\\n",
    "# np.var(transmitter_0(b_0) - transmitter_7(b_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86bf836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.var(transmitter_0._tb_encoder(b_0) - transmitter_1._tb_encoder(b_0)), np.var(transmitter_0._tb_encoder(b_0) - transmitter_2._tb_encoder(b_0)),\\\n",
    "# np.var(transmitter_0._tb_encoder(b_0) - transmitter_3._tb_encoder(b_0)), np.var(transmitter_0._tb_encoder(b_0) - transmitter_4._tb_encoder(b_0)),\\\n",
    "# np.var(transmitter_0._tb_encoder(b_0) - transmitter_5._tb_encoder(b_0)), np.var(transmitter_0._tb_encoder(b_0) - transmitter_6._tb_encoder(b_0)),\\\n",
    "# np.var(transmitter_0._tb_encoder(b_0) - transmitter_7._tb_encoder(b_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34703c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pusch_config_8 = pusch_config_0.clone()\n",
    "# pusch_config_8.tb.mcs_index = 5\n",
    "# transmitter_8 = PUSCHTransmitter(pusch_config_8, return_bits=False)\n",
    "# x_8 = transmitter_8._mapper(c_0)\n",
    "# x_8 = transmitter_8._layer_mapper(x_8)\n",
    "# x_8 = transmitter_8._resource_grid_mapper(x_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8f15dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.var(x_0-x_1), np.var(x_0-x_2), np.var(x_0-x_3), np.var(x_0-x_4), np.var(x_0-x_5), np.var(x_0-x_6), np.var(x_0-x_7), np.var(x_0-x_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d09cce56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nframe_number, n_cell_id: No change\\nn_rnti, tb.n_id: No change in tb_encoder (same b -> same c), but in dmrs\\nslot_number, dmrs.n_id, dmrs.n_scid, mcs: Change in tb_encoder, but not in dmrs (same c -> same x)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "frame_number, n_cell_id: No change\n",
    "n_rnti, tb.n_id: No change in tb_encoder (same b -> same c), but in dmrs\n",
    "slot_number, dmrs.n_id, dmrs.n_scid, mcs: Change in tb_encoder, but not in dmrs (same c -> same x)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5decf6b8",
   "metadata": {},
   "source": [
    "Next, we test if this receiver works over a simple Rayleigh block fading channel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8db340ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLA can lead to reduced numerical precision. Use with care.\n"
     ]
    }
   ],
   "source": [
    "# We need to enable sionna.config.xla_compat before we can use\n",
    "# tf.function with jit_compile=True.\n",
    "# See https://nvlabs.github.io/sionna/api/config.html#sionna.Config.xla_compat\n",
    "sionna.config.xla_compat=True\n",
    "\n",
    "class MyGenerator(tf.keras.layers.Layer):\n",
    "    def __init__(self, pusch_config, channel_model, mode=1, dir='../Dataset'):\n",
    "        super().__init__()\n",
    "        # Create a PUSCH configuration with default settings\n",
    "        self.dir = dir\n",
    "        os.makedirs(self.dir, exist_ok=True)\n",
    "        self._pusch_config = pusch_config\n",
    "\n",
    "        self.tb_size = pusch_config.tb_size\n",
    "        self.num_coded_bits = pusch_config.num_coded_bits\n",
    "        self.num_subcarriers = 12*pusch_config.n_size_bwp\n",
    "        self.num_symbols = pusch_config.symbol_allocation[1]\n",
    "        self.dmrs_symbol_indices = pusch_config.dmrs_symbol_indices\n",
    "\n",
    "        self._pusch_transmitter = PUSCHTransmitter(pusch_config)\n",
    "                \n",
    "        \n",
    "        rx_tx_association = np.ones([1, 1], bool)\n",
    "        stream_management = StreamManagement(rx_tx_association,1)   \n",
    "\n",
    "        detector = MaximumLikelihoodDetector(output=\"bit\",\n",
    "                                             demapping_method=\"maxlog\",\n",
    "                                             resource_grid=self._pusch_transmitter.resource_grid,\n",
    "                                             stream_management=stream_management,\n",
    "                                             constellation_type=\"qam\",\n",
    "                                             num_bits_per_symbol=pusch_config.tb.num_bits_per_symbol)\n",
    "            \n",
    "        if mode==0:\n",
    "            self._pusch_receiver = PUSCHReceiver(self._pusch_transmitter,\n",
    "                                                 mimo_detector=detector,\n",
    "                                                 channel_estimator=\"perfect\")\n",
    "        else:\n",
    "            self._pusch_receiver = PUSCHReceiver(self._pusch_transmitter)\n",
    "        \n",
    "\n",
    "        file_path = os.path.join(f'{self.dir}', f'pusch_transmiter.txt')\n",
    "        with open(file_path, 'w') as f:\n",
    "            self._pusch_transmitter.show(f)\n",
    "    \n",
    "        self._channel = OFDMChannel(channel_model,\n",
    "                            self._pusch_transmitter.resource_grid,\n",
    "                            return_channel=True)\n",
    "\n",
    "        self.save_to_dataset = [*list('bcxhy'),'b_hat']\n",
    "\n",
    "\n",
    "    def poll(self):\n",
    "        existing_samples = 0\n",
    "        if os.path.isfile(f'{self.dir}/b.bin'):\n",
    "            existing_samples = os.path.getsize(f'{self.dir}/b.bin') * 8 // self.num_coded_bits\n",
    "        return existing_samples\n",
    "        \n",
    "    def call(self, batch_size, ebno_db):\n",
    "        if self.save_to_dataset:\n",
    "            no_dir = f'{self.dir}'\n",
    "            os.makedirs(no_dir, exist_ok=True)\n",
    "\n",
    "        x, b, c = self._pusch_transmitter(batch_size, ret=['c'])\n",
    "        \n",
    "        n = ebnodb2no(ebno_db,\n",
    "                    self._pusch_transmitter._num_bits_per_symbol,\n",
    "                    self._pusch_transmitter._target_coderate,\n",
    "                    self._pusch_transmitter.resource_grid)\n",
    "        \n",
    "        y, h = self._channel([x, n])\n",
    "        \n",
    "        if 'b' in self.save_to_dataset:\n",
    "            bbin = os.path.join(f'{no_dir}', f'b.bin')\n",
    "            with open(bbin, 'ab') as file:\n",
    "                file.write(b2b(tf.pad(b, tf.constant([[0,0],[0,0],[0, self.num_coded_bits - self.tb_size]]))))\n",
    "                \n",
    "        if 'c' in self.save_to_dataset:\n",
    "            cbin = os.path.join(f'{no_dir}', f'c.bin')\n",
    "            with open(cbin, 'ab') as file:\n",
    "                file.write(b2b(c))\n",
    "\n",
    "        if 'x' in self.save_to_dataset:\n",
    "            xbin = os.path.join(f'{no_dir}', f'x.bin')\n",
    "            with open(xbin, 'ab') as file:\n",
    "                file.write(f2f(tf.stack((tf.math.real(x), tf.math.imag(x)), axis=-1)))\n",
    "\n",
    "        if 'h' in self.save_to_dataset:\n",
    "            hbin = os.path.join(f'{no_dir}', f'h.bin')\n",
    "            with open(hbin, 'ab') as file:\n",
    "                file.write(f2f(tf.stack((tf.math.real(h), tf.math.imag(h)), axis=-1)))\n",
    "\n",
    "        if 'y' in self.save_to_dataset:        \n",
    "            ybin = os.path.join(f'{no_dir}', f'y.bin')\n",
    "            with open(ybin, 'ab') as file:\n",
    "                file.write(f2f(tf.stack((tf.math.real(y), tf.math.imag(y)), axis=-1)))\n",
    "\n",
    "        # if 'n' in self.save_to_dataset:    \n",
    "        #     batch_n = tf.repeat(n, repeats=batch_size)\n",
    "        #     nbin = os.path.join(f'{no_dir}', f'n.bin')\n",
    "        #     with open(nbin, 'ab') as file:\n",
    "        #         file.write(tf.cast((batch_n * (2 ** 6)), tf.int8))\n",
    "            \n",
    "        if self._pusch_receiver._perfect_csi:\n",
    "            b_hat = self._pusch_receiver([y, h, n]) \n",
    "        else:\n",
    "            b_hat = self._pusch_receiver([y, n])\n",
    "\n",
    "        if 'b_hat' in self.save_to_dataset:\n",
    "            b_hatbin = os.path.join(f'{no_dir}', f'b_hat.bin')\n",
    "            with open(b_hatbin, 'ab') as file:\n",
    "                file.write(b2b(tf.pad(b_hat, tf.constant([[0,0],[0,0],[0, self.num_coded_bits - self.tb_size]]))))\n",
    "        \n",
    "        return b, b_hat\n",
    "\n",
    "class MyLoader(tf.keras.layers.Layer):\n",
    "    def __init__(self, generator):\n",
    "        super().__init__()\n",
    "        # Create a PUSCH configuration with default settings\n",
    "        self.dir = generator.dir\n",
    "\n",
    "        self.tb_size = generator.tb_size\n",
    "        self.num_coded_bits = generator.num_coded_bits\n",
    "        self.num_subcarriers = generator.num_subcarriers\n",
    "        self.num_symbols = generator.num_symbols\n",
    "        self.dmrs_symbol_indices = generator.dmrs_symbol_indices\n",
    "        # Instantiate a PUSCHTransmitter from the PUSCHConfig                  \n",
    "\n",
    "        self.dataset = tf.data.Dataset.from_generator(self.load, \n",
    "                        output_signature=(\n",
    "                        tf.TensorSpec(shape=(1, self.tb_size), dtype=tf.float32),\n",
    "                        tf.TensorSpec(shape=(1, self.tb_size), dtype=tf.float32),\n",
    "                        tf.TensorSpec(shape=(1, self.num_coded_bits), dtype=tf.float32),\n",
    "                        tf.TensorSpec(shape=(1, 1, self.num_symbols, self.num_subcarriers), dtype=tf.complex64),\n",
    "                        tf.TensorSpec(shape=(1, 8, 1, 1, self.num_symbols, self.num_subcarriers), dtype=tf.complex64),\n",
    "                        tf.TensorSpec(shape=(1, 8, self.num_symbols, self.num_subcarriers), dtype=tf.complex64),\n",
    "                        # tf.TensorSpec(shape=(), dtype=tf.float32)\n",
    "                        ))       \n",
    "        \n",
    "        self.model_dataset = tf.data.Dataset.from_generator(self.load2trainer, \n",
    "                        output_signature=(\n",
    "                        tf.TensorSpec(shape=(1, 1, self.num_symbols-2, self.num_subcarriers, 2), dtype=tf.uint8),\n",
    "                        tf.TensorSpec(shape=(1, 8, self.num_symbols-2, self.num_subcarriers, 2), dtype=tf.int16),\n",
    "                        # tf.TensorSpec(shape=(), dtype=tf.float32)\n",
    "                        )) \n",
    "\n",
    "    def load(self):\n",
    "        no_dir = f'{self.dir}'\n",
    "\n",
    "        with open(os.path.join(no_dir, 'b.bin'), \"rb\") as bfile, \\\n",
    "                open(os.path.join(no_dir, 'b_hat.bin'), \"rb\") as b_hatfile, \\\n",
    "                open(os.path.join(no_dir, 'c.bin'), \"rb\") as cfile, \\\n",
    "                open(os.path.join(no_dir, 'x.bin'), \"rb\") as xfile, \\\n",
    "                open(os.path.join(no_dir, 'h.bin'), \"rb\") as hfile, \\\n",
    "                open(os.path.join(no_dir, 'y.bin'), \"rb\") as yfile:\n",
    "                # open(os.path.join(no_dir, 'n.bin'), \"rb\") as nfile:\n",
    "        \n",
    "            while True:\n",
    "                b_batch = np.frombuffer(bfile.read(1 * self.num_coded_bits // 8), dtype=np.uint8)\n",
    "                b_hat_batch = np.frombuffer(b_hatfile.read(1 * self.num_coded_bits // 8), dtype=np.uint8)\n",
    "                c_batch = np.frombuffer(cfile.read(1 * self.num_coded_bits // 8), dtype=np.uint8)\n",
    "                x_batch = np.frombuffer(xfile.read(1 * self.num_symbols * self.num_subcarriers * 2 * 2), dtype=np.int16)\n",
    "                h_batch = np.frombuffer(hfile.read(1 * 8 * self.num_symbols * self.num_subcarriers * 2 * 2), dtype=np.int16)\n",
    "                y_batch = np.frombuffer(yfile.read(1 * 8 * self.num_symbols * self.num_subcarriers * 2 * 2), dtype=np.int16)\n",
    "                # n_batch = np.frombuffer(nfile.read(1), dtype=np.int8)\n",
    "\n",
    "                if not b_batch.size: break  # Stop when reaching the end\n",
    "\n",
    "                b = tf.cast(np.unpackbits(b_batch).reshape(1, self.num_coded_bits),tf.float32)[:,:self.tb_size]\n",
    "                b_hat = tf.cast(np.unpackbits(b_hat_batch).reshape(1, self.num_coded_bits),tf.float32)[:,:self.tb_size]\n",
    "                c = tf.cast(np.unpackbits(c_batch).reshape(1, self.num_coded_bits),tf.float32)\n",
    "                x = (x_batch.reshape(1, 1, self.num_symbols, self.num_subcarriers, 2))\n",
    "                x = tf.cast((x[..., 0] + 1j * x[..., 1]) / (2**13), tf.complex64)\n",
    "                h = (h_batch.reshape(1, 8, 1, 1, self.num_symbols, self.num_subcarriers, 2))\n",
    "                h = tf.cast((h[..., 0] + 1j * h[..., 1]) / (2**13), tf.complex64)\n",
    "                y = (y_batch.reshape(1, 8, self.num_symbols, self.num_subcarriers, 2))\n",
    "                y = tf.cast((y[..., 0] + 1j * y[..., 1]) / (2**13), tf.complex64)\n",
    "                # n = tf.cast(n_batch / (2**6), tf.float32)[0]\n",
    "                yield b, b_hat, c, x, h, y\n",
    "\n",
    "    def load2trainer(self):\n",
    "        no_dir = f'{self.dir}'\n",
    "\n",
    "        with open(os.path.join(no_dir, 'c.bin'), \"rb\") as cfile, \\\n",
    "                open(os.path.join(no_dir, 'y.bin'), \"rb\") as yfile:\n",
    "                # open(os.path.join(no_dir, 'n.bin'), \"rb\") as nfile:\n",
    "        \n",
    "            while True:\n",
    "                c_batch = np.frombuffer(cfile.read(1 * self.num_coded_bits // 8), dtype=np.uint8)\n",
    "                y_batch = np.frombuffer(yfile.read(1 * 8 * self.num_symbols * self.num_subcarriers * 2 * 2), dtype=np.int16)\n",
    "                # n_batch = np.frombuffer(nfile.read(1), dtype=np.int8)\n",
    "\n",
    "                if not c_batch.size: break  # Stop when reaching the end\n",
    "                c = np.unpackbits(c_batch).reshape(1, 1, self.num_symbols - 2, self.num_subcarriers, 2)\n",
    "                y = (y_batch.reshape(1, 8, self.num_symbols, self.num_subcarriers, 2))\n",
    "                y = tf.concat([y[:,:,0:2],y[:,:,4:12],y[:,:,12:14]],axis=2)\n",
    "\n",
    "                # n = tf.cast(n_batch[0]/(2**6),tf.float32)\n",
    "\n",
    "                yield c, y\n",
    "generator = MyGenerator(pusch_config_0, channel_model_0, dir='/workspaces/thanh/Dataset/sfasf')\n",
    "loader = MyLoader(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "300508d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcss = [5]\n",
    "cdl_models = [\"A\"]\n",
    "speeds = [10]\n",
    "delay_spreads = [150e-9]\n",
    "# pusch_slots = [4,5,14,15]\n",
    "ebno_dbs = [-8.0]\n",
    "\n",
    "# num_samples = 1000\n",
    "num_cases = len(cdl_models) * len(speeds) * len(delay_spreads) * len(mcss) * len(ebno_dbs)\n",
    "\n",
    "# num_samples_per_param = num_samples // num_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98835049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mu = 1\n",
    "# num_ofdm_symbols = 14\n",
    "# fft_size = 4096\n",
    "# cyclic_prefix_length = 288\n",
    "# subcarrier_spacing = 30e3\n",
    "# num_guard_subcarriers = (410, 410)\n",
    "# num_slots_per_frame = 20\n",
    "\n",
    "# resource_grid = ResourceGrid(\n",
    "#     num_ofdm_symbols=num_ofdm_symbols,\n",
    "#     fft_size=fft_size,\n",
    "#     subcarrier_spacing=subcarrier_spacing,\n",
    "#     num_tx=1,\n",
    "#     num_streams_per_tx=1,\n",
    "#     cyclic_prefix_length=cyclic_prefix_length,\n",
    "#     num_guard_carriers=num_guard_subcarriers,\n",
    "#     dc_null=False,\n",
    "#     pilot_pattern=None,\n",
    "#     pilot_ofdm_symbol_indices=None\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08b0f07",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# transmitters = {}\n",
    "# receivers = {}\n",
    "# nos = {}\n",
    "# pusch_config_0 = MyPUSCHConfig()\n",
    "# transmitter_0 = PUSCHTransmitter(pusch_config_0)\n",
    "\n",
    "# for mcs, slot_number in itertools.product(mcss, pusch_slots):\n",
    "#     pusch_config_i = pusch_config_0.clone()\n",
    "#     pusch_config_i.tb.mcs_index = mcs\n",
    "#     pusch_config_i.carrier.slot_number = slot_number\n",
    "#     transmitters[mcs, slot_number] = PUSCHTransmitter(pusch_config_i)\n",
    "#     receivers[mcs, slot_number] = PUSCHReceiver(transmitters[mcs, slot_number])\n",
    "    \n",
    "# resource_grid = transmitter_0.resource_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8e4a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fff3ab5ae28403f90b2f7d7ab3f5aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_dir = f'../Dataset/{datetime.now().strftime(\"%Y-%m-%d\")}/{datetime.now().strftime(\"%H-%M-%S\")}'\n",
    "start = time.time()\n",
    "\n",
    "PUSCH_SIMS = {\n",
    "    \"bler\" : [],\n",
    "    \"ber\" : [],\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "pusch_config_0 = MyPUSCHConfig()\n",
    "# transmitter_0 = PUSCHTransmitter(pusch_config_0)\n",
    "\n",
    "for (mcs, cdl_model, speed, delay_spread, ebno_db) in \\\n",
    "        (pbar := tqdm(itertools.product(mcss, cdl_models, speeds, delay_spreads, ebno_dbs), total=num_cases)):\n",
    "    status_str = f\"Generating... (MCS: {mcs} | CDL-{cdl_model} | {speed} m/s | {delay_spread} s | ebno_db {ebno_db})\"\n",
    "    pbar.set_description(status_str)\n",
    "    \n",
    "    channel_model = CDL(cdl_model, delay_spread, carrier_frequency, ue_antenna, gnb_array, \"uplink\", min_speed=speed)\n",
    "    dir = f'{data_dir}/MCS_{mcs}_CDL_{cdl_model}_speed_{speed}_ds{delay_spread*1e9}_ebno_db_{ebno_db}'\n",
    "    \n",
    "    pusch_config_i = pusch_config_0.clone()\n",
    "    pusch_config_i.tb.mcs_index = mcs\n",
    "    generator = MyGenerator(pusch_config_i, channel_model, mode=1, dir=dir)\n",
    "    ber, bler = sim_ber(generator,\n",
    "                    tf.constant([ebno_db]),\n",
    "                    batch_size=16,\n",
    "                    max_mc_iter=80,\n",
    "                    early_stop=False)\n",
    "    PUSCH_SIMS[\"ber\"].append(list(ber.numpy()))\n",
    "    PUSCH_SIMS[\"bler\"].append(list(bler.numpy()))\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ad298b2",
   "metadata": {},
   "source": [
    "Hopefully you have enjoyed this tutorial on Sionna's 5G NR PUSCH module!\n",
    "\n",
    "Please have a look at the [API documentation](https://nvlabs.github.io/sionna/api/sionna.html) of the various components or the other available [tutorials](https://nvlabs.github.io/sionna/tutorials.html) to learn more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
