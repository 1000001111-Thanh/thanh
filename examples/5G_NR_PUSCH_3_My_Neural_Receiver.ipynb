{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1e3c39cc",
      "metadata": {
        "id": "1e3c39cc"
      },
      "source": [
        "# Neural Receiver for OFDM SIMO Systems"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69d31188",
      "metadata": {
        "id": "69d31188"
      },
      "source": [
        "## GPU Configuration and Imports <a class=\"anchor\" id=\"GPU-Configuration-and-Imports\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QYiZ55UuAl22",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYiZ55UuAl22",
        "outputId": "0b7c4918-5483-45b0-8e01-21218cf559a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'thanh'...\n",
            "remote: Enumerating objects: 5594, done.\u001b[K\n",
            "remote: Counting objects: 100% (71/71), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 5594 (delta 54), reused 48 (delta 48), pack-reused 5523 (from 3)\u001b[K\n",
            "Receiving objects: 100% (5594/5594), 153.23 MiB | 16.95 MiB/s, done.\n",
            "Resolving deltas: 100% (4092/4092), done.\n",
            "Updating files: 100% (3763/3763), done.\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/1000001111/thanh.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "v4cyjy7jBFqj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4cyjy7jBFqj",
        "outputId": "9559e537-736d-4962-8932-96306c6f21ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow<2.16.0,>=2.13.0 (from -r /content/thanh/requirements.txt (line 1))\n",
            "  Downloading tensorflow-2.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r /content/thanh/requirements.txt (line 2)) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from -r /content/thanh/requirements.txt (line 3)) (1.13.1)\n",
            "Requirement already satisfied: matplotlib>=3.5.3 in /usr/local/lib/python3.11/dist-packages (from -r /content/thanh/requirements.txt (line 4)) (3.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from -r /content/thanh/requirements.txt (line 5)) (6.5.2)\n",
            "Collecting mitsuba<3.6.0,>=3.2.0 (from -r /content/thanh/requirements.txt (line 6))\n",
            "  Downloading mitsuba-3.5.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting pythreejs>=2.4.2 (from -r /content/thanh/requirements.txt (line 7))\n",
            "  Downloading pythreejs-2.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting ipywidgets>=8.0.4 (from -r /content/thanh/requirements.txt (line 8))\n",
            "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting ipydatawidgets==4.3.2 (from -r /content/thanh/requirements.txt (line 9))\n",
            "  Downloading ipydatawidgets-4.3.2-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting jupyterlab-widgets==3.0.5 (from -r /content/thanh/requirements.txt (line 10))\n",
            "  Downloading jupyterlab_widgets-3.0.5-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: traittypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipydatawidgets==4.3.2->-r /content/thanh/requirements.txt (line 9)) (0.2.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1)) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1)) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1)) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1)) (18.1.1)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1))\n",
            "  Downloading ml_dtypes-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1)) (4.25.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1)) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1)) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1)) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1)) (4.12.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1))\n",
            "  Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1)) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1)) (1.70.0)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1))\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1))\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1))\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.3->-r /content/thanh/requirements.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.3->-r /content/thanh/requirements.txt (line 4)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.3->-r /content/thanh/requirements.txt (line 4)) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.3->-r /content/thanh/requirements.txt (line 4)) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.3->-r /content/thanh/requirements.txt (line 4)) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.3->-r /content/thanh/requirements.txt (line 4)) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.3->-r /content/thanh/requirements.txt (line 4)) (2.8.2)\n",
            "Collecting drjit==0.4.6 (from mitsuba<3.6.0,>=3.2.0->-r /content/thanh/requirements.txt (line 6))\n",
            "  Downloading drjit-0.4.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.11/dist-packages (from pythreejs>=2.4.2->-r /content/thanh/requirements.txt (line 7)) (5.7.1)\n",
            "Collecting comm>=0.1.3 (from ipywidgets>=8.0.4->-r /content/thanh/requirements.txt (line 8))\n",
            "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->-r /content/thanh/requirements.txt (line 8)) (7.34.0)\n",
            "Collecting widgetsnbextension~=4.0.12 (from ipywidgets>=8.0.4->-r /content/thanh/requirements.txt (line 8))\n",
            "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
            "INFO: pip is looking at multiple versions of ipywidgets to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting ipywidgets>=8.0.4 (from -r /content/thanh/requirements.txt (line 8))\n",
            "  Downloading ipywidgets-8.1.4-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading ipywidgets-8.1.3-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Downloading ipywidgets-8.1.2-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Downloading ipywidgets-8.1.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Downloading ipywidgets-8.1.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Downloading ipywidgets-8.0.7-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->-r /content/thanh/requirements.txt (line 8)) (6.17.1)\n",
            "  Downloading ipywidgets-8.0.6-py3-none-any.whl.metadata (2.4 kB)\n",
            "INFO: pip is still looking at multiple versions of ipywidgets to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading ipywidgets-8.0.5-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1)) (0.45.1)\n",
            "Collecting jedi>=0.16 (from ipython>=6.1.0->ipywidgets>=8.0.4->-r /content/thanh/requirements.txt (line 8))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->-r /content/thanh/requirements.txt (line 8)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->-r /content/thanh/requirements.txt (line 8)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->-r /content/thanh/requirements.txt (line 8)) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->-r /content/thanh/requirements.txt (line 8)) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->-r /content/thanh/requirements.txt (line 8)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->-r /content/thanh/requirements.txt (line 8)) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->-r /content/thanh/requirements.txt (line 8)) (4.9.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1)) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1)) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1)) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1)) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1)) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1)) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->-r /content/thanh/requirements.txt (line 8)) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->-r /content/thanh/requirements.txt (line 8)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.4->-r /content/thanh/requirements.txt (line 8)) (0.2.13)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1)) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1)) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1)) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->-r /content/thanh/requirements.txt (line 1)) (3.2.2)\n",
            "Downloading ipydatawidgets-4.3.2-py2.py3-none-any.whl (271 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m271.6/271.6 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_widgets-3.0.5-py3-none-any.whl (384 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.3/384.3 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.3/475.3 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mitsuba-3.5.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (40.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading drjit-0.4.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pythreejs-2.4.2-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipywidgets-8.0.5-py3-none-any.whl (138 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, widgetsnbextension, tensorflow-estimator, ml-dtypes, keras, jupyterlab-widgets, jedi, drjit, mitsuba, ipywidgets, tensorboard, ipydatawidgets, tensorflow, pythreejs\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: widgetsnbextension\n",
            "    Found existing installation: widgetsnbextension 3.6.10\n",
            "    Uninstalling widgetsnbextension-3.6.10:\n",
            "      Successfully uninstalled widgetsnbextension-3.6.10\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.8.0\n",
            "    Uninstalling keras-3.8.0:\n",
            "      Successfully uninstalled keras-3.8.0\n",
            "  Attempting uninstall: jupyterlab-widgets\n",
            "    Found existing installation: jupyterlab_widgets 3.0.13\n",
            "    Uninstalling jupyterlab_widgets-3.0.13:\n",
            "      Successfully uninstalled jupyterlab_widgets-3.0.13\n",
            "  Attempting uninstall: ipywidgets\n",
            "    Found existing installation: ipywidgets 7.7.1\n",
            "    Uninstalling ipywidgets-7.7.1:\n",
            "      Successfully uninstalled ipywidgets-7.7.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.18.0\n",
            "    Uninstalling tensorflow-2.18.0:\n",
            "      Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.15.1 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.15.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed drjit-0.4.6 ipydatawidgets-4.3.2 ipywidgets-8.0.5 jedi-0.19.2 jupyterlab-widgets-3.0.5 keras-2.15.0 mitsuba-3.5.2 ml-dtypes-0.3.2 pythreejs-2.4.2 tensorboard-2.15.2 tensorflow-2.15.1 tensorflow-estimator-2.15.0 widgetsnbextension-4.0.13 wrapt-1.14.1\n"
          ]
        }
      ],
      "source": [
        "# !pip install -r /content/thanh/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pTHjHPceBQNJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTHjHPceBQNJ",
        "outputId": "08c581b5-038e-4528-c61e-5d40b842c806"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6393b2fe",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-26T14:51:41.881148Z",
          "iopub.status.busy": "2024-09-26T14:51:41.880894Z",
          "iopub.status.idle": "2024-09-26T14:51:44.498124Z",
          "shell.execute_reply": "2024-09-26T14:51:44.497219Z"
        },
        "id": "6393b2fe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if os.getenv(\"CUDA_VISIBLE_DEVICES\") is None:\n",
        "    gpu_num = 0 # Use \"\" to use the CPU\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_num}\"\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "\n",
        "import sys\n",
        "# sys.path.append('/content/thanh/')\n",
        "sys.path.append('../')\n",
        "import sionna\n",
        "# Import Sionna\n",
        "# try:\n",
        "#     import sionna\n",
        "# except ImportError as e:\n",
        "#     # Install Sionna if package is not already installed\n",
        "#     import os\n",
        "#     os.system(\"pip install sionna\")\n",
        "#     import sionna\n",
        "\n",
        "# Configure the notebook to use only a single GPU and allocate only as much memory as needed\n",
        "# For more details, see https://www.tensorflow.org/guide/gpu\n",
        "import tensorflow as tf\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "# Avoid warnings from TensorFlow\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "sionna.config.seed = 42 # Set seed for reproducible random number generation\n",
        "\n",
        "# Load the required Sionna components\n",
        "from sionna.nr import PUSCHConfig, PUSCHTransmitter, PUSCHReceiver, CarrierConfig, PUSCHDMRSConfig,\\\n",
        "                        TBConfig, PUSCHPilotPattern, TBEncoder, PUSCHPrecoder, LayerMapper, LayerDemapper, check_pusch_configs,\\\n",
        "                        TBDecoder, PUSCHLSChannelEstimator\n",
        "from sionna.nr.utils import generate_prng_seq\n",
        "from sionna.channel import AWGN, RayleighBlockFading, OFDMChannel, TimeChannel, time_lag_discrete_time_channel\n",
        "from sionna.channel.utils import *\n",
        "from sionna.channel.tr38901 import Antenna, AntennaArray, UMi, UMa, RMa, TDL, CDL\n",
        "from sionna.channel import gen_single_sector_topology as gen_topology\n",
        "from sionna.utils import compute_ber, ebnodb2no, sim_ber, array_to_hash, create_timestamped_folders, b2b, f2f, BinarySource\n",
        "from sionna.ofdm import KBestDetector, LinearDetector, MaximumLikelihoodDetector,\\\n",
        "        LSChannelEstimator, LMMSEEqualizer, RemoveNulledSubcarriers, ResourceGridDemapper,\\\n",
        "        ResourceGrid, ResourceGridMapper, OFDMModulator\n",
        "from sionna.mimo import StreamManagement\n",
        "from sionna.mapping import Mapper, Demapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "DJ2Hf2rj-i6B",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJ2Hf2rj-i6B",
        "outputId": "aa18496b-8790-4483-96e4-1a49f4d09ac1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "print(gpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6244a108",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-26T14:51:44.501712Z",
          "iopub.status.busy": "2024-09-26T14:51:44.501431Z",
          "iopub.status.idle": "2024-09-26T14:51:44.511679Z",
          "shell.execute_reply": "2024-09-26T14:51:44.511010Z"
        },
        "id": "6244a108"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "# from bs4 import BeautifulSoup\n",
        "import pickle\n",
        "from collections import namedtuple\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "import io"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10b3af73",
      "metadata": {
        "id": "10b3af73"
      },
      "source": [
        "## Simulation Parameters <a class=\"anchor\" id=\"Simulation-Parameters\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2e2b69eb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-26T14:51:44.514347Z",
          "iopub.status.busy": "2024-09-26T14:51:44.514184Z",
          "iopub.status.idle": "2024-09-26T14:51:44.518257Z",
          "shell.execute_reply": "2024-09-26T14:51:44.517619Z"
        },
        "id": "2e2b69eb"
      },
      "outputs": [],
      "source": [
        "# _num_tx = 1\n",
        "# _num_rx = 1\n",
        "# _num_tx_ant = 1\n",
        "# _num_rx_ant = 8\n",
        "# _carrier_frequency = 2.55e9  # Carrier frequency in Hz.\n",
        "# _link_direction = \"uplink\"\n",
        "\n",
        "# # Configure antenna arrays\n",
        "# _ue_antenna = Antenna(polarization=\"single\",\n",
        "#                 polarization_type=\"V\",\n",
        "#                 antenna_pattern=\"38.901\",\n",
        "#                 carrier_frequency=_carrier_frequency)\n",
        "\n",
        "# _gnb_array = AntennaArray(num_rows=1,\n",
        "#                         num_cols=_num_rx_ant//2,\n",
        "#                         polarization=\"dual\",\n",
        "#                         polarization_type=\"cross\",\n",
        "#                         antenna_pattern=\"38.901\",\n",
        "#                         carrier_frequency=_carrier_frequency)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "Y-Uxb7N9DnJ_",
      "metadata": {
        "id": "Y-Uxb7N9DnJ_"
      },
      "outputs": [],
      "source": [
        "class MyPUSCHConfig(PUSCHConfig):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            carrier_config=CarrierConfig(\n",
        "                n_cell_id=0,\n",
        "                cyclic_prefix=\"normal\",\n",
        "                subcarrier_spacing=30,\n",
        "                n_size_grid=273,\n",
        "                n_start_grid=0,\n",
        "                slot_number=4,\n",
        "                frame_number=0\n",
        "            ),\n",
        "            pusch_dmrs_config=PUSCHDMRSConfig(\n",
        "                config_type=1,\n",
        "                length=1,\n",
        "                additional_position=1,\n",
        "                dmrs_port_set=[0],\n",
        "                n_id=0,\n",
        "                n_scid=0,\n",
        "                num_cdm_groups_without_data=2,\n",
        "                type_a_position=2\n",
        "            ),\n",
        "            tb_config=TBConfig(\n",
        "                channel_type='PUSCH',\n",
        "                n_id=0,\n",
        "                mcs_table=1,\n",
        "                mcs_index=9\n",
        "            ),\n",
        "            mapping_type='A',\n",
        "            n_size_bwp=273,\n",
        "            n_start_bwp=0,\n",
        "            num_layers=1,\n",
        "            num_antenna_ports=1,\n",
        "            precoding='non-codebook',\n",
        "            tpmi=0,\n",
        "            transform_precoding=False,\n",
        "            n_rnti=2008,\n",
        "            symbol_allocation=[0,14]\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "GNFemRNCsCvS",
      "metadata": {
        "id": "GNFemRNCsCvS"
      },
      "outputs": [],
      "source": [
        "def load_pickle(filename):\n",
        "    \"\"\"Saves data to a pickle file.\"\"\"\n",
        "    with open(filename, \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def data_loader(df, pickes_dir):\n",
        "    #  # .sample(frac=1) for shuffing\n",
        "    for pusch_record in df.sample(frac=1).itertuples():\n",
        "        data_dirname = pusch_record.Data_dirname\n",
        "        data_filename = pusch_record.Data_filename\n",
        "        esno_db = pusch_record.Esno_db\n",
        "        index = pusch_record.Index\n",
        "\n",
        "        # 1 tx\n",
        "        b = load_pickle(f'{pickes_dir}/{data_dirname}/{data_filename}.b.pkl')[0]\n",
        "        c = load_pickle(f'{pickes_dir}/{data_dirname}/{data_filename}.c.pkl')[0]\n",
        "        y = load_pickle(f'{pickes_dir}/{data_dirname}/{data_filename}.y.pkl')[0]\n",
        "\n",
        "        c_len = tf.shape(c)[-1]\n",
        "        b_len = tf.shape(b)[-1]\n",
        "\n",
        "        b = tf.pad(b, [[0,0],[0,c_len-b_len]])  # Pad b with zeros to match c\n",
        "        # b_mask = tf.concat([tf.ones(b_len, dtype=tf.uint8), tf.zeros(c_len - b_len, dtype=tf.uint8)], axis=-1)\n",
        "\n",
        "        yield esno_db, c, y, b, b_len\n",
        "\n",
        "# Processing input data to train with full Data Resource Grid\n",
        "def preprocessing_data_grid(esno_db, c, y, b, b_len):\n",
        "    c = c[0]\n",
        "    y = y[0]\n",
        "    b = b[0]\n",
        "\n",
        "    No = 10**(-esno_db / 10.)\n",
        "\n",
        "    c = tf.transpose(tf.reshape(c, [12,-1,2]), perm=[1,0,2])\n",
        "\n",
        "    # Remove zero padding and 2 dmrs symbol\n",
        "    # y = tf.concat([y[...,0:2,410:-410],y[...,3:11,410:-410], y[...,12:14,410:-410]],axis=-2)\n",
        "    y = y[...,410:-410]\n",
        "\n",
        "    # Concat Real and Image of y\n",
        "    y = tf.concat([tf.math.real(y), tf.math.imag(y)], axis=0)\n",
        "\n",
        "    y = tf.transpose(y, perm=[2,1,0])\n",
        "\n",
        "    return No, c, y, b, b_len\n",
        "\n",
        "\n",
        "# Processing input data to train with n Data RB\n",
        "def preprocessing_2(esno_db, c, y, b, b_len):\n",
        "    # c: (2, 12, 3276)\n",
        "    # y: (16, 12, 3276)\n",
        "    No, c, y, b, b_len = preprocessing_data_grid(esno_db, c, y, b, b_len)\n",
        "\n",
        "    # c: (273, 12, 12, 2)\n",
        "    # y: (273, 12, 12, 16)\n",
        "    c = tf.transpose(tf.reshape(c, [2,12, -1, 12]), perm=[2,1,3,0])\n",
        "    y = tf.transpose(tf.reshape(y, [16,12, -1, 12]), perm=[2,1,3,0])\n",
        "\n",
        "    y = tf.cast(y*(2**13), tf.int16)\n",
        "    # c = tf.transpose(c, perm=[2,1,0])\n",
        "    # y = tf.transpose(y, perm=[2,1,0])\n",
        "\n",
        "    return No, c, y, b, b_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZbL-oU5fsNB6",
      "metadata": {
        "id": "ZbL-oU5fsNB6"
      },
      "outputs": [],
      "source": [
        "pickles_dir = '/content/drive/MyDrive/Pusch_data/pickle'\n",
        "parquet_path = '/content/drive/MyDrive/Pusch_data/parquet/20250219001504217540.parquet'\n",
        "BATCH_SIZE = 2\n",
        "df = pd.read_parquet(parquet_path, engine=\"pyarrow\")\n",
        "# df = df[(df['nMCS'] == 9) & (df['nSlot'] == 4)]\n",
        "# df =df.sample(NUM_SAMPLE)\n",
        "\n",
        "# Create Dataloader Instance\n",
        "dataset = tf.data.Dataset.from_generator(\n",
        "            lambda: data_loader(df, pickles_dir),\n",
        "            # (ebno_db, c, y, b, b_len)\n",
        "            output_types=(tf.float32, tf.float32, tf.complex64, tf.float32, tf.int32))\n",
        "\n",
        "# dataset = dataset.cache()\n",
        "# dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "dataset = dataset.map(preprocessing_data_grid).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "siO0XT0PDn0S",
      "metadata": {
        "id": "siO0XT0PDn0S"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Layer, Conv2D, LayerNormalization, SeparableConv2D\n",
        "from tensorflow.nn import relu\n",
        "class ResidualBlock(tf.keras.Model):\n",
        "    r\"\"\"\n",
        "    This Keras layer implements a convolutional residual block made of two convolutional layers with ReLU activation, layer normalization, and a skip connection.\n",
        "    The number of convolutional channels of the input must match the number of kernel of the convolutional layers ``num_conv_channel`` for the skip connection to work.\n",
        "\n",
        "    Input\n",
        "    ------\n",
        "    : [batch size, num time samples, num subcarriers, num_conv_channel], tf.float\n",
        "        Input of the layer\n",
        "\n",
        "    Output\n",
        "    -------\n",
        "    : [batch size, num time samples, num subcarriers, num_conv_channel], tf.float\n",
        "        Output of the layer\n",
        "    \"\"\"\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        # Layer normalization is done over the last three dimensions: time, frequency, conv 'channels'\n",
        "        self._layer_norm_1 = LayerNormalization(axis=(-1, -2, -3))\n",
        "        self._conv_1 = SeparableConv2D(filters= 64,\n",
        "                              kernel_size=[3,3],\n",
        "                              padding='same',\n",
        "                              activation=None)\n",
        "        # Layer normalization is done over the last three dimensions: time, frequency, conv 'channels'\n",
        "        self._layer_norm_2 = LayerNormalization(axis=(-1, -2, -3))\n",
        "        self._conv_2 = SeparableConv2D(filters= 128,\n",
        "                              kernel_size=[3,3],\n",
        "                              padding='same',\n",
        "                              activation=None)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z = self._layer_norm_1(inputs)\n",
        "        z = relu(z)\n",
        "        z = self._conv_1(z)\n",
        "        z = self._layer_norm_2(z)\n",
        "        z = relu(z)\n",
        "        z = self._conv_2(z) # [batch size, num time samples, num subcarriers, num_channels]\n",
        "        # Skip connection\n",
        "        z = z + inputs\n",
        "\n",
        "        return z\n",
        "\n",
        "class CustomNeuralReceiver(tf.keras.Model):\n",
        "    r\"\"\"\n",
        "    Keras layer implementing a residual convolutional neural receiver.\n",
        "\n",
        "    This neural receiver is fed with the post-DFT received samples, forming a resource grid of size num_of_symbols x fft_size, and computes LLRs on the transmitted coded bits.\n",
        "    These LLRs can then be fed to an outer decoder to reconstruct the information bits.\n",
        "\n",
        "    Input\n",
        "    ------\n",
        "    y_no: [batch size, num ofdm symbols, num subcarriers, 2*num rx antenna + 1], tf.float32\n",
        "        Concatenated received samples and noise variance.\n",
        "(\n",
        "    y : [batch size, num rx antenna, num ofdm symbols, num subcarriers], tf.complex\n",
        "        Received post-DFT samples.\n",
        "\n",
        "    no : [batch size], tf.float32\n",
        "        Noise variance. At training, a different noise variance value is sampled for each batch example.\n",
        ")\n",
        "    Output\n",
        "    -------\n",
        "    : [batch size, num ofdm symbols, num subcarriers, num_bits_per_symbol]\n",
        "        LLRs on the transmitted bits.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, training = False):\n",
        "        super(CustomNeuralReceiver, self).__init__()\n",
        "        self._training = training\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        # Input convolution\n",
        "        self._input_conv = Conv2D(filters= 128,\n",
        "                                  kernel_size=[3,3],\n",
        "                                  padding='same',\n",
        "                                  activation=None)\n",
        "        # Residual blocks\n",
        "        self._res_block_1 = ResidualBlock()\n",
        "        self._res_block_2 = ResidualBlock()\n",
        "        self._res_block_3 = ResidualBlock()\n",
        "        self._res_block_4 = ResidualBlock()\n",
        "        # Output conv\n",
        "        self._output_conv = Conv2D(filters= 2,    # QPSK\n",
        "                                   kernel_size=[3,3],\n",
        "                                   padding='same',\n",
        "                                   activation=None)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Input conv\n",
        "        z = self._input_conv(inputs)\n",
        "        # Residual blocks\n",
        "        z = self._res_block_1(z)\n",
        "        z = self._res_block_2(z)\n",
        "        z = self._res_block_3(z)\n",
        "        z = self._res_block_4(z)\n",
        "        # Output conv\n",
        "        z = self._output_conv(z)\n",
        "        # if self._training == False:\n",
        "        #     z = tf.cast(z * (2**7), tf.int8)\n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "b8FPAeC1fJZL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8FPAeC1fJZL",
        "outputId": "cba5eed3-cd63-42d9-db3b-630a94519b09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"custom_neural_receiver_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_2 (Conv2D)           multiple                  18560     \n",
            "                                                                 \n",
            " residual_block_4 (Residual  multiple                  17630080  \n",
            " Block)                                                          \n",
            "                                                                 \n",
            " residual_block_5 (Residual  multiple                  17630080  \n",
            " Block)                                                          \n",
            "                                                                 \n",
            " residual_block_6 (Residual  multiple                  17630080  \n",
            " Block)                                                          \n",
            "                                                                 \n",
            " residual_block_7 (Residual  multiple                  17630080  \n",
            " Block)                                                          \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           multiple                  2306      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 70541186 (269.09 MB)\n",
            "Trainable params: 70541186 (269.09 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Loaded pretrained weights from /workspaces/thanh/Checkpoint/model_weight_FULL_RB_epoch_40.pkl\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 1, 1, 78624), dtype=float32, numpy=\n",
              "array([[[[ 0.39727205, -0.06281994, -0.63921374, ...,  0.00379648,\n",
              "          -0.2373026 , -0.23420796]]]], dtype=float32)>"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "_model = CustomNeuralReceiver(training = False)\n",
        "inputs = tf.zeros([1,3276,14,16])\n",
        "_model(inputs)\n",
        "_model.summary()\n",
        "\n",
        "def load_weights(model, pretrained_weights_path):\n",
        "    # Build Model with random input\n",
        "    # Load weights\n",
        "  with open(pretrained_weights_path, 'rb') as f:\n",
        "    weights = pickle.load(f)\n",
        "    model.set_weights(weights)\n",
        "    print(f\"Loaded pretrained weights from {pretrained_weights_path}\")\n",
        "\n",
        "load_weights(_model, '/workspaces/thanh/Checkpoint/model_weight_FULL_RB_epoch_40.pkl')\n",
        "\n",
        "llr = _model(inputs)\n",
        "llr = tf.concat([llr[...,0:2,:],llr[...,3:11,:], llr[...,12:14,:]],axis=-2)\n",
        "llr = tf.transpose(llr, [0, 2, 1, 3])\n",
        "llr = tf.reshape(llr, [tf.shape(llr)[0], 1, 1, -1])\n",
        "llr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "wL3ETYFGuc29",
      "metadata": {
        "id": "wL3ETYFGuc29"
      },
      "outputs": [],
      "source": [
        "# for (No, c, y, b, b_len) in  dataset:\n",
        "#     break\n",
        "# # No, c, y, b, b_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "-B5d57oFtszF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "-B5d57oFtszF",
        "outputId": "acadf7ae-98cb-457d-d17c-37f8bcb6a1fd"
      },
      "outputs": [],
      "source": [
        "# df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "Rp-JbwScs9WR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rp-JbwScs9WR",
        "outputId": "b0107632-8249-47f5-9379-2281eca8983f"
      },
      "outputs": [],
      "source": [
        "# pred = _model(y)\n",
        "# pred = tf.concat([pred[...,0:2,:], pred[...,3:11,:], pred[...,12:14,:]],axis=-2)\n",
        "# bce = tf.nn.sigmoid_cross_entropy_with_logits(c, pred)\n",
        "# bce = tf.reduce_mean(bce, axis=[1,2,3])\n",
        "# bce, No"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "IpfDB1ovFzW8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpfDB1ovFzW8",
        "outputId": "0d86b0ea-00a7-4cc9-f1d3-cde69014f353"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XLA can lead to reduced numerical precision. Use with care.\n"
          ]
        }
      ],
      "source": [
        "# We need to enable sionna.config.xla_compat before we can use\n",
        "# tf.function with jit_compile=True.\n",
        "# See https://nvlabs.github.io/sionna/api/config.html#sionna.Config.xla_compat\n",
        "sionna.config.xla_compat=True\n",
        "\n",
        "class Model(tf.keras.Model):\n",
        "    def __init__(self, decoding_scenario, channel_scenario):\n",
        "        super().__init__()\n",
        "\n",
        "        assert decoding_scenario in ['perfect-csi', 'ls-without-prior', 'neural-receiver']\n",
        "        self.decoding_scenario = decoding_scenario\n",
        "        channel_scenario = channel_scenario.split('-')\n",
        "        assert len(channel_scenario) == 4\n",
        "        (chnl, ml, ds, spd) = channel_scenario\n",
        "        ds = int(ds)\n",
        "        spd = int(spd)\n",
        "        assert chnl == 'CDL'\n",
        "\n",
        "        self._pusch_config = MyPUSCHConfig()\n",
        "        self._pusch_config.tb.mcs_index = 3\n",
        "\n",
        "\n",
        "        self._tb_size = self._pusch_config.tb_size\n",
        "        self._num_coded_bits = self._pusch_config.num_coded_bits\n",
        "        self._target_coderate = self._pusch_config.tb.target_coderate\n",
        "        self._num_bits_per_symbol = self._pusch_config.tb.num_bits_per_symbol\n",
        "        _num_layers = self._pusch_config.num_layers\n",
        "        _n_rnti = self._pusch_config.n_rnti\n",
        "        _n_id = self._pusch_config.tb.n_id\n",
        "\n",
        "\n",
        "        self._binary_source = BinarySource(dtype=tf.float32)\n",
        "\n",
        "        self._tb_encoder = TBEncoder(\n",
        "                        target_tb_size=self._tb_size,\n",
        "                        num_coded_bits=self._num_coded_bits,\n",
        "                        target_coderate=self._target_coderate,\n",
        "                        num_bits_per_symbol=self._num_bits_per_symbol,\n",
        "                        num_layers=_num_layers,\n",
        "                        n_rnti=_n_rnti,\n",
        "                        n_id=_n_id,\n",
        "                        channel_type=\"PUSCH\", # PUSCHTransmitter\n",
        "                        codeword_index=0, # not supported for PUSCH\n",
        "                        use_scrambler=True,\n",
        "                        verbose=False,\n",
        "                        output_dtype=tf.float32)\n",
        "\n",
        "        self._mapper = Mapper(\"qam\", self._num_bits_per_symbol, dtype=tf.complex64)\n",
        "\n",
        "        self._layer_mapper = LayerMapper(num_layers=_num_layers, dtype=tf.complex64)\n",
        "\n",
        "        _dmrs_length = self._pusch_config.dmrs.length\n",
        "        _dmrs_additional_position = self._pusch_config.dmrs.additional_position\n",
        "        _num_cdm_groups_without_data = self._pusch_config.dmrs.num_cdm_groups_without_data\n",
        "        _n_scid = self._pusch_config.dmrs.n_scid\n",
        "        _n_id_n_scid = self._pusch_config.dmrs.n_id[0]\n",
        "\n",
        "        _pilot_pattern = PUSCHPilotPattern([self._pusch_config],\n",
        "                                                dtype=tf.complex64)\n",
        "        _mu = 1\n",
        "        _num_ofdm_symbols = 14\n",
        "        _fft_size = 4096\n",
        "        _cyclic_prefix_length = 288\n",
        "        _subcarrier_spacing = 30e3\n",
        "        _num_guard_subcarriers = (410, 410)\n",
        "        _num_slots_per_frame = 20\n",
        "\n",
        "        # Define the resource grid.\n",
        "        self._resource_grid = ResourceGrid(\n",
        "            num_ofdm_symbols=_num_ofdm_symbols,\n",
        "            fft_size=_fft_size,\n",
        "            subcarrier_spacing=_subcarrier_spacing,\n",
        "            num_tx=1,\n",
        "            num_streams_per_tx=1,\n",
        "            cyclic_prefix_length=_cyclic_prefix_length,\n",
        "            num_guard_carriers=_num_guard_subcarriers,\n",
        "            dc_null=False,\n",
        "            pilot_pattern=_pilot_pattern,\n",
        "            dtype=tf.complex64\n",
        "        )\n",
        "\n",
        "        self._resource_grid_mapper = ResourceGridMapper(self._resource_grid, dtype=tf.complex64)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self._num_tx = 1\n",
        "        _num_rx = 1\n",
        "        _num_tx_ant = 1\n",
        "        _num_rx_ant = 8\n",
        "        _carrier_frequency = 2.55e9  # Carrier frequency in Hz.\n",
        "        _link_direction = \"uplink\"\n",
        "        _rx_tx_association = np.ones([_num_rx, self._num_tx], bool)\n",
        "        _stream_management = StreamManagement(_rx_tx_association, _num_layers)\n",
        "\n",
        "\n",
        "        if 'neural-receiver' == decoding_scenario:\n",
        "            self._model = _model\n",
        "        else:\n",
        "            if 'ls-without-prior' == decoding_scenario:\n",
        "                self._channel_estimator = PUSCHLSChannelEstimator(\n",
        "                                self._resource_grid,\n",
        "                                _dmrs_length,\n",
        "                                _dmrs_additional_position,\n",
        "                                _num_cdm_groups_without_data,\n",
        "                                interpolation_type='lin',\n",
        "                                dtype=tf.complex64)\n",
        "\n",
        "            self._mimo_detector = LinearDetector(\"lmmse\", \"bit\", \"maxlog\", self._resource_grid,\n",
        "                                            _stream_management, \"qam\",self. _num_bits_per_symbol, dtype=tf.complex64)\n",
        "\n",
        "\n",
        "        self._layer_demapper = LayerDemapper(self._layer_mapper, num_bits_per_symbol=self._num_bits_per_symbol)\n",
        "        self._tb_decoder = TBDecoder(self._tb_encoder, output_dtype=tf.float32)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Configure antenna arrays\n",
        "        _ue_antenna = Antenna(polarization=\"single\",\n",
        "                        polarization_type=\"V\",\n",
        "                        antenna_pattern=\"38.901\",\n",
        "                        carrier_frequency=_carrier_frequency)\n",
        "\n",
        "        _gnb_array = AntennaArray(num_rows=1,\n",
        "                                num_cols=_num_rx_ant//2,\n",
        "                                polarization=\"dual\",\n",
        "                                polarization_type=\"cross\",\n",
        "                                antenna_pattern=\"38.901\",\n",
        "                                carrier_frequency=_carrier_frequency)\n",
        "\n",
        "        self._channel_model = CDL(model = ml,\n",
        "                                  delay_spread = ds*1e-9,\n",
        "                                  carrier_frequency = _carrier_frequency,\n",
        "                                  ut_array = _ue_antenna,\n",
        "                                  bs_array = _gnb_array,\n",
        "                                  direction = _link_direction,\n",
        "                                  min_speed = spd,\n",
        "                                  max_speed = spd)\n",
        "\n",
        "        self._channel = OFDMChannel(\n",
        "                            self._channel_model,\n",
        "                            self._resource_grid,\n",
        "                            # normalize_channel=True,\n",
        "                            return_channel=True)\n",
        "\n",
        "    def predict_with_model(self, y, batch_size, num_coded_bits):\n",
        "        def prep_y(y):\n",
        "            # c = tf.transpose(tf.reshape(c, [c.shape[0], 12,-1,2]), perm=[0,2,1,3])\n",
        "            y = y[...,410:-410]\n",
        "\n",
        "            # Concat Real and Image of y\n",
        "            y = tf.concat([tf.math.real(y), tf.math.imag(y)], axis=2)\n",
        "            y = y[:,0]\n",
        "            y = tf.transpose(y, perm=[0,3,2,1])\n",
        "            return y\n",
        "\n",
        "        # y.shape, c.shape\n",
        "        y_prep = prep_y(y)\n",
        "        pred = self._model(y_prep)\n",
        "        pred = tf.concat([pred[...,0:2,:], pred[...,3:11,:], pred[...,12:14,:]],axis=-2)\n",
        "        pred = tf.transpose(pred, perm=[0,2,1,3])\n",
        "        c_pred = tf.reshape(pred, [batch_size,1,1,num_coded_bits])\n",
        "\n",
        "        return c_pred\n",
        "\n",
        "    # def new_topology(self, batch_size):\n",
        "    #     \"\"\"Set new topology\"\"\"\n",
        "    #     topology = gen_topology(batch_size,\n",
        "    #                             self._num_tx,\n",
        "    #                             'umi',\n",
        "    #                             min_ut_velocity=0,\n",
        "    #                             max_ut_velocity=0)\n",
        "\n",
        "    #     self._channel_model.set_topology(*topology)\n",
        "\n",
        "    @tf.function(jit_compile=True)\n",
        "    def call(self, batch_size, ebno_db):\n",
        "        # self.new_topology(batch_size)\n",
        "\n",
        "\n",
        "\n",
        "        b = self._binary_source([batch_size, self._num_tx, self._tb_size])\n",
        "        c = self._tb_encoder(b)\n",
        "        x_map = self._mapper(c)\n",
        "        x_layer = self._layer_mapper(x_map)\n",
        "        x = self._resource_grid_mapper(x_layer)\n",
        "\n",
        "        \n",
        "        no = ebnodb2no(ebno_db,\n",
        "                       self._num_bits_per_symbol,\n",
        "                       self._target_coderate,\n",
        "                       self._resource_grid)\n",
        "        y, h = self._channel([x, no])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        if 'neural-receiver' == self.decoding_scenario:\n",
        "            llr_det = self.predict_with_model(y, batch_size, self._num_coded_bits)\n",
        "        else:\n",
        "            if 'ls-without-prior' == self.decoding_scenario:\n",
        "                no_arg = 0.001\n",
        "                h_hat,err_var = self._channel_estimator([y, no_arg])\n",
        "\n",
        "            elif 'perfect-csi':\n",
        "                h_hat = h[...,410:-410]\n",
        "                no_arg = no\n",
        "                err_var = tf.cast(0, tf.float32)\n",
        "\n",
        "            llr_det = self._mimo_detector([y, h_hat, err_var, no_arg])\n",
        "\n",
        "\n",
        "\n",
        "        # print(llr_det)\n",
        "        llr_layer = self._layer_demapper(llr_det)\n",
        "\n",
        "        # bce = tf.nn.sigmoid_cross_entropy_with_logits(c, llr_layer)\n",
        "        # bce = tf.reduce_mean(bce)\n",
        "\n",
        "\n",
        "\n",
        "        b_hat, tb_crc_status = self._tb_decoder(llr_layer)\n",
        "\n",
        "        return b, b_hat\n",
        "    \n",
        "e2e = Model('neural-receiver', 'CDL-A-150-10')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "Sf7HkpstyJwu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "Sf7HkpstyJwu",
        "outputId": "527bb9ee-b910-46bc-df00-16e4ee7c6a13"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float64, numpy=0.0>"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "b,b_hat = e2e(2,-5.)\n",
        "compute_ber(b,b_hat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "ff4648f4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.19 s ± 65.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%timeit e2e(2,-5.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "9d30a02f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(2, 1, 19464), dtype=float32, numpy=\n",
              " array([[[0., 0., 0., ..., 1., 1., 1.]],\n",
              " \n",
              "        [[1., 0., 1., ..., 1., 0., 0.]]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(2, 1, 19464), dtype=float32, numpy=\n",
              " array([[[0., 0., 0., ..., 1., 1., 1.]],\n",
              " \n",
              "        [[1., 0., 1., ..., 1., 0., 0.]]], dtype=float32)>)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "e2e(2,-5.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W_BLBAStLWuJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_BLBAStLWuJ",
        "outputId": "fac10498-fee3-4dbd-bb34-b3b971a314e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Channel scenario:  CDL-A-150-10\n",
            "Decoding_scenario:  neural-receiver\n",
            "EbNo [dB] |        BER |       BLER |  bit errors |    num bits | block errors |  num blocks | runtime [s] |    status\n",
            "---------------------------------------------------------------------------------------------------------------------------------------\n",
            "    -10.0 | 2.2820e-01 | 7.0833e-01 |      213198 |      934272 |           34 |          48 |       206.1 |iter: 2/100\r"
          ]
        }
      ],
      "source": [
        "channel_scenarios = ['CDL-A-150-10', 'CDL-B-150-10']\n",
        "decoding_scenarios = ['neural-receiver', 'ls-without-prior', 'perfect-csi']\n",
        "PUSCH_SIMS = {\n",
        "    \"bler\" : [],\n",
        "    \"ber\" : []\n",
        "    }\n",
        "\n",
        "start = time.time()\n",
        "ebno_dbs = np.arange(-10., 15., 1)\n",
        "for channel_scenario in channel_scenarios:\n",
        "    print('Channel scenario: ', channel_scenario)\n",
        "    for decoding_scenario in decoding_scenarios:\n",
        "        print('Decoding_scenario: ', decoding_scenario)\n",
        "        e2e = Model(decoding_scenario, channel_scenario)\n",
        "        ber, bler = sim_ber(e2e, ebno_dbs, batch_size=16, num_target_block_errors=256, max_mc_iter=100)\n",
        "        PUSCH_SIMS[\"ber\"].append(list(ber.numpy()))\n",
        "        PUSCH_SIMS[\"bler\"].append(list(bler.numpy()))\n",
        "\n",
        "PUSCH_SIMS[\"duration\"] = time.time() - start"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
