{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiswkh5AR5V3",
        "outputId": "07f60711-ea26-4d14-cbe0-7510473cd0a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Run two below lines to drive'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"Run two below lines to get my_abc module\"\"\"\n",
        "# !git clone https://github.com/thanhttttt/thanh.git\n",
        "# !pip install -r /content/thanh/requirements.txt\n",
        "\"\"\"Run two below lines to drive\"\"\"\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PVBTrEZrR4kL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if os.getenv(\"CUDA_VISIBLE_DEVICES\") is None:\n",
        "    gpu_num = 0 # Use \"\" to use the CPU\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_num}\"\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.path.append('../')\n",
        "sys.path.append('/content/thanh/')\n",
        "sys.path.append('../thanh/')\n",
        "\n",
        "import sionna\n",
        "\n",
        "import tensorflow as tf\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "# Avoid warnings from TensorFlow\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "sionna.config.seed = 42 # Set seed for reproducible random number generation\n",
        "\n",
        "# Load the required Sionna components\n",
        "from sionna.nr.my_abc import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load model weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"custom_neural_receiver\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             multiple                  20864     \n",
            "                                                                 \n",
            " residual_block (ResidualBl  multiple                  639232    \n",
            " ock)                                                            \n",
            "                                                                 \n",
            " residual_block_1 (Residual  multiple                  639232    \n",
            " Block)                                                          \n",
            "                                                                 \n",
            " residual_block_2 (Residual  multiple                  639232    \n",
            " Block)                                                          \n",
            "                                                                 \n",
            " residual_block_3 (Residual  multiple                  639232    \n",
            " Block)                                                          \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           multiple                  2306      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2580098 (9.84 MB)\n",
            "Trainable params: 2580098 (9.84 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Loaded pretrained weights from ../weight_4RB_UMI_dynamic_config.pkl\n"
          ]
        }
      ],
      "source": [
        "_model = CustomNeuralReceiver(training = False)\n",
        "inputs = tf.zeros([1,48,14,18])\n",
        "_model(inputs)\n",
        "_model.summary()\n",
        "\n",
        "#load_weights(_model, '/content/drive/MyDrive/Pusch_data/Model_weights/model_weight_FULL_RB_epoch_40.pkl')\n",
        "# load_weights(_model, '../model_weight_FULL_RB_epoch_40.pkl')\n",
        "load_weights(_model, '../weight_4RB_UMI_dynamic_config.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfT7Ao7seGqP"
      },
      "source": [
        "# Set up config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"test setup\"\"\"\n",
        "batch_size = 8\n",
        "\n",
        "\"\"\"nrb config setup\"\"\"\n",
        "RB_start = 0\n",
        "NRB = 162\n",
        "PCI = 443\n",
        "RNTI = 40035\n",
        "MCS = 8\n",
        "\n",
        "\"\"\"channel setup\"\"\"\n",
        "no = 2.\n",
        "CDL_model = 'A'\n",
        "delay_spread = 50\n",
        "speed = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## create channel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "channel_model = CDL(model = CDL_model,\n",
        "                            delay_spread = delay_spread*1e-9,\n",
        "                            carrier_frequency = CARRIER_FREQUENCY,\n",
        "                            ut_array = Ue_Antenna,\n",
        "                            bs_array = Gnb_AntennaArray,\n",
        "                            direction = 'uplink',\n",
        "                            min_speed = speed,\n",
        "                            max_speed = speed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## create 4RB samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"default config is 4RB\"\"\"\n",
        "sysCfg = SystemConfig()\n",
        "ueCfg = UeConfig()\n",
        "myCfg = MyConfig(sysCfg, [ueCfg])\n",
        "puschCfg = MyPUSCHConfig(myCfg)\n",
        "# puschCfg.show() # uncomment for detail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "simulator = MySimulator(puschCfg)\n",
        "channel = OFDMChannel(channel_model=channel_model, resource_grid=simulator.resource_grid,\n",
        "                                    add_awgn=False, normalize_channel=True, return_channel=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "b, c, y, x ,h = simulator.sim(batch_size, channel, no, return_tx_iq=True, return_channel=True)\n",
        "r = simulator.ref(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## create N-RB samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"test setup\"\"\"\n",
        "batch_size = 1\n",
        "\n",
        "\"\"\"nrb config setup\"\"\"\n",
        "RB_start = 22\n",
        "NRB = 140\n",
        "PCI = 442\n",
        "RNTI = 30025\n",
        "MCS = 0\n",
        "\n",
        "\"\"\"channel setup\"\"\"\n",
        "no = 2.\n",
        "CDL_model = 'A'\n",
        "delay_spread = 50\n",
        "speed = 1\n",
        "sysCfg = SystemConfig(\n",
        "                    NCellId = PCI,\n",
        "                    FrequencyRange = 1,\n",
        "                    BandWidth = 60,\n",
        "                    Numerology = 1,\n",
        "                    CpType = 0,\n",
        "                    NTxAnt = 1,\n",
        "                    NRxAnt = 8,\n",
        "                    BwpNRb = 162,\n",
        "                    BwpRbOffset = 0,\n",
        "                    harqProcFlag = 0,\n",
        "                    nHarqProc = 1,\n",
        "                    rvSeq = 0\n",
        "                )\n",
        "ueCfg = UeConfig(\n",
        "                TransformPrecoding = 0,\n",
        "                Rnti = RNTI,\n",
        "                nId = PCI,\n",
        "                CodeBookBased = 0,\n",
        "                DmrsPortSetIdx = [0],\n",
        "                NLayers = 1,\n",
        "                NumDmrsCdmGroupsWithoutData = 2,\n",
        "                Tpmi = 0,\n",
        "                FirstSymb = 0,\n",
        "                NPuschSymbAll = 14,\n",
        "                RaType = 1,\n",
        "                FirstPrb = RB_start,\n",
        "                NPrb = NRB,\n",
        "                FrequencyHoppingMode = 0,\n",
        "                McsTable = 0,\n",
        "                Mcs = MCS,\n",
        "                ILbrm = 0,\n",
        "                nScId = 0,\n",
        "                NnScIdId = PCI,\n",
        "                DmrsConfigurationType = 0,\n",
        "                DmrsDuration = 1,\n",
        "                DmrsAdditionalPosition = 1,\n",
        "                PuschMappingType = 0,\n",
        "                DmrsTypeAPosition = 3,\n",
        "                HoppingMode = 0,\n",
        "                NRsId = 0,\n",
        "                Ptrs = 0,\n",
        "                ScalingFactor = 0,\n",
        "                OAck = 0,\n",
        "                IHarqAckOffset = 11,\n",
        "                OCsi1 = 0,\n",
        "                ICsi1Offset = 7,\n",
        "                OCsi2 = 0,\n",
        "                ICsi2Offset = 0,\n",
        "                NPrbOh = 0,\n",
        "                nCw = 1,\n",
        "                TpPi2Bpsk = 0\n",
        "            )\n",
        "myCfg = MyConfig(sysCfg, [ueCfg])\n",
        "puschCfg = MyPUSCHConfig(myCfg, 4)\n",
        "# puschCfg.show() # uncomment for detail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "#\n",
        "# SPDX-FileCopyrightText: Copyright (c) 2021-2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
        "# SPDX-License-Identifier: Apache-2.0\n",
        "#\n",
        "\"\"\"Layers for LDPC channel encoding and utility functions.\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "from tensorflow.keras.layers import Layer\n",
        "from importlib_resources import files, as_file\n",
        "from sionna.fec.ldpc import codes # pylint: disable=relative-beyond-top-level\n",
        "import numbers # to check if n, k are numbers\n",
        "\n",
        "from sionna.fec.linear import AllZeroEncoder as AllZeroEncoder_new\n",
        "\n",
        "class LDPC5GEncoder(Layer):\n",
        "    # pylint: disable=line-too-long\n",
        "    \"\"\"LDPC5GEncoder(k, n, num_bits_per_symbol=None, dtype=tf.float32, **kwargs)\n",
        "\n",
        "    5G NR LDPC Encoder following the 3GPP NR Initiative [3GPPTS38212_LDPC]_\n",
        "    including rate-matching.\n",
        "\n",
        "    The class inherits from the Keras layer class and can be used as layer in a\n",
        "    Keras model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        k: int\n",
        "            Defining the number of information bit per codeword.\n",
        "\n",
        "        n: int\n",
        "            Defining the desired codeword length.\n",
        "\n",
        "        num_bits_per_symbol: int or None\n",
        "            Defining the number of bits per QAM symbol. If this parameter is\n",
        "            explicitly provided, the codeword will be interleaved after\n",
        "            rate-matching as specified in Sec. 5.4.2.2 in [3GPPTS38212_LDPC]_.\n",
        "\n",
        "        dtype: tf.DType\n",
        "            Defaults to `tf.float32`. Defines the output datatype of the layer\n",
        "            (internal precision remains `tf.uint8`).\n",
        "\n",
        "    Input\n",
        "    -----\n",
        "        inputs: [...,k], tf.float32\n",
        "            2+D tensor containing the information bits to be\n",
        "            encoded.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        : [...,n], tf.float32\n",
        "            2+D tensor of same shape as inputs besides last dimension has\n",
        "            changed to `n` containing the encoded codeword bits.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "        k: int\n",
        "            Defining the number of information bit per codeword.\n",
        "\n",
        "        n: int\n",
        "            Defining the desired codeword length.\n",
        "\n",
        "        coderate: float\n",
        "            Defining the coderate r= ``k`` / ``n``.\n",
        "\n",
        "        n_ldpc: int\n",
        "            An integer defining the total codeword length (before\n",
        "            punturing) of the lifted parity-check matrix.\n",
        "\n",
        "        k_ldpc: int\n",
        "            An integer defining the total information bit length\n",
        "            (before zero removal) of the lifted parity-check matrix. Gap to\n",
        "            ``k`` must be filled with so-called filler bits.\n",
        "\n",
        "        num_bits_per_symbol: int or None.\n",
        "            Defining the number of bits per QAM symbol. If this parameter is\n",
        "            explicitly provided, the codeword will be interleaved after\n",
        "            rate-matching as specified in Sec. 5.4.2.2 in [3GPPTS38212_LDPC]_.\n",
        "\n",
        "        out_int: [n], ndarray of int\n",
        "            Defining the rate-matching output interleaver sequence.\n",
        "\n",
        "        out_int_inv: [n], ndarray of int\n",
        "            Defining the inverse rate-matching output interleaver sequence.\n",
        "\n",
        "        _check_input: bool\n",
        "            A boolean that indicates whether the input vector\n",
        "            during call of the layer should be checked for consistency (i.e.,\n",
        "            binary).\n",
        "\n",
        "        _bg: str\n",
        "            Denoting the selected basegraph (either `bg1` or `bg2`).\n",
        "\n",
        "        _z: int\n",
        "            Denoting the lifting factor.\n",
        "\n",
        "        _i_ls: int\n",
        "            Defining which version of the basegraph to load.\n",
        "            Can take values between 0 and 7.\n",
        "\n",
        "        _k_b: int\n",
        "            Defining the number of `information bit columns` in the\n",
        "            basegraph. Determined by the code design procedure in\n",
        "            [3GPPTS38212_LDPC]_.\n",
        "\n",
        "        _bm: ndarray\n",
        "            An ndarray defining the basegraph.\n",
        "\n",
        "        _pcm: sp.sparse.csr_matrix\n",
        "            A sparse matrix of shape `[k_ldpc-n_ldpc, n_ldpc]`\n",
        "            containing the sparse parity-check matrix.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "        AssertionError\n",
        "            If ``k`` is not `int`.\n",
        "\n",
        "        AssertionError\n",
        "            If ``n`` is not `int`.\n",
        "\n",
        "        ValueError\n",
        "            If ``code_length`` is not supported.\n",
        "\n",
        "        ValueError\n",
        "            If `dtype` is not supported.\n",
        "\n",
        "        ValueError\n",
        "            If ``inputs`` contains other values than `0` or `1`.\n",
        "\n",
        "        InvalidArgumentError\n",
        "            When rank(``inputs``)<2.\n",
        "\n",
        "        InvalidArgumentError\n",
        "            When shape of last dim is not ``k``.\n",
        "\n",
        "    Note\n",
        "    ----\n",
        "        As specified in [3GPPTS38212_LDPC]_, the encoder also performs\n",
        "        puncturing and shortening. Thus, the corresponding decoder needs to\n",
        "        `invert` these operations, i.e., must be compatible with the 5G\n",
        "        encoding scheme.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 k,\n",
        "                 n,\n",
        "                 num_bits_per_symbol=None,\n",
        "                 dtype=tf.float32,\n",
        "                 **kwargs):\n",
        "\n",
        "        super().__init__(dtype=dtype, **kwargs)\n",
        "\n",
        "        assert isinstance(k, numbers.Number), \"k must be a number.\"\n",
        "        assert isinstance(n, numbers.Number), \"n must be a number.\"\n",
        "        k = int(k) # k or n can be float (e.g. as result of n=k*r)\n",
        "        n = int(n) # k or n can be float (e.g. as result of n=k*r)\n",
        "\n",
        "        if dtype is not tf.float32:\n",
        "            print(\"Note: decoder uses tf.float32 for internal calculations.\")\n",
        "\n",
        "        if dtype not in (tf.float16, tf.float32, tf.float64, tf.int8,\n",
        "            tf.int32, tf.int64, tf.uint8, tf.uint16, tf.uint32):\n",
        "            raise ValueError(\"Unsupported dtype.\")\n",
        "        self._dtype = dtype\n",
        "\n",
        "        if k>8448:\n",
        "            raise ValueError(\"Unsupported code length (k too large).\")\n",
        "        if k<12:\n",
        "            raise ValueError(\"Unsupported code length (k too small).\")\n",
        "\n",
        "        if n>(316*384):\n",
        "            raise ValueError(\"Unsupported code length (n too large).\")\n",
        "        if n<0:\n",
        "            raise ValueError(\"Unsupported code length (n negative).\")\n",
        "\n",
        "        # init encoder parameters\n",
        "        self._k = k # number of input bits (= input shape)\n",
        "        self._n = n # the desired length (= output shape)\n",
        "        self._coderate = k / n\n",
        "        self._check_input = True # check input for consistency (i.e., binary)\n",
        "\n",
        "        # allow actual code rates slightly larger than 948/1024\n",
        "        # to account for the quantization procedure in 38.214 5.1.3.1\n",
        "        if self._coderate>(948/1024): # as specified in 38.212 5.4.2.1\n",
        "            print(f\"Warning: effective coderate r>948/1024 for n={n}, k={k}.\")\n",
        "        if self._coderate>(0.95): # as specified in 38.212 5.4.2.1\n",
        "            raise ValueError(f\"Unsupported coderate (r>0.95) for n={n}, k={k}.\")\n",
        "        # if self._coderate<(1/5):\n",
        "        #     # outer rep. coding currently not supported\n",
        "        #     raise ValueError(\"Unsupported coderate (r<1/5).\")\n",
        "\n",
        "        # construct the basegraph according to 38.212\n",
        "        self._bg = self._sel_basegraph(self._k, self._coderate)\n",
        "        self._z, self._i_ls, self._k_b = self._sel_lifting(self._k, self._bg)\n",
        "        self._bm = self._load_basegraph(self._i_ls, self._bg)\n",
        "\n",
        "        # total number of codeword bits\n",
        "        self._n_ldpc = self._bm.shape[1] * self._z\n",
        "        # if K_real < K _target puncturing must be applied earlier\n",
        "        self._k_ldpc = self._k_b * self._z\n",
        "\n",
        "        # construct explicit graph via lifting\n",
        "        pcm = self._lift_basegraph(self._bm, self._z)\n",
        "\n",
        "        pcm_a, pcm_b_inv, pcm_c1, pcm_c2 = self._gen_submat(self._bm,\n",
        "                                                            self._k_b,\n",
        "                                                            self._z,\n",
        "                                                            self._bg)\n",
        "\n",
        "        # init sub-matrices for fast encoding (\"RU\"-method)\n",
        "        # note: dtype is tf.float32;\n",
        "        self._pcm = pcm # store the sparse parity-check matrix (for decoding)\n",
        "\n",
        "        # store indices for fast gathering (instead of explicit matmul)\n",
        "        self._pcm_a_ind = self._mat_to_ind(pcm_a)\n",
        "        self._pcm_b_inv_ind = self._mat_to_ind(pcm_b_inv)\n",
        "        self._pcm_c1_ind = self._mat_to_ind(pcm_c1)\n",
        "        self._pcm_c2_ind = self._mat_to_ind(pcm_c2)\n",
        "\n",
        "        self._num_bits_per_symbol = num_bits_per_symbol\n",
        "        if num_bits_per_symbol is not None:\n",
        "            self._out_int, self._out_int_inv  = self.generate_out_int(self._n,\n",
        "                                                    self._num_bits_per_symbol)\n",
        "\n",
        "    #########################################\n",
        "    # Public methods and properties\n",
        "    #########################################\n",
        "\n",
        "    @property\n",
        "    def k(self):\n",
        "        \"\"\"Number of input information bits.\"\"\"\n",
        "        return self._k\n",
        "\n",
        "    @property\n",
        "    def n(self):\n",
        "        \"Number of output codeword bits.\"\n",
        "        return self._n\n",
        "\n",
        "    @property\n",
        "    def coderate(self):\n",
        "        \"\"\"Coderate of the LDPC code after rate-matching.\"\"\"\n",
        "        return self._coderate\n",
        "\n",
        "    @property\n",
        "    def k_ldpc(self):\n",
        "        \"\"\"Number of LDPC information bits after rate-matching.\"\"\"\n",
        "        return self._k_ldpc\n",
        "\n",
        "    @property\n",
        "    def n_ldpc(self):\n",
        "        \"\"\"Number of LDPC codeword bits before rate-matching.\"\"\"\n",
        "        return self._n_ldpc\n",
        "\n",
        "    @property\n",
        "    def pcm(self):\n",
        "        \"\"\"Parity-check matrix for given code parameters.\"\"\"\n",
        "        return self._pcm\n",
        "\n",
        "    @property\n",
        "    def z(self):\n",
        "        \"\"\"Lifting factor of the basegraph.\"\"\"\n",
        "        return self._z\n",
        "\n",
        "    @property\n",
        "    def num_bits_per_symbol(self):\n",
        "        \"\"\"Modulation order used for the rate-matching output interleaver.\"\"\"\n",
        "        return self._num_bits_per_symbol\n",
        "\n",
        "    @property\n",
        "    def out_int(self):\n",
        "        \"\"\"Output interleaver sequence as defined in 5.4.2.2.\"\"\"\n",
        "        return self._out_int\n",
        "    @property\n",
        "    def out_int_inv(self):\n",
        "        \"\"\"Inverse output interleaver sequence as defined in 5.4.2.2.\"\"\"\n",
        "        return self._out_int_inv\n",
        "\n",
        "    #########################\n",
        "    # Utility methods\n",
        "    #########################\n",
        "\n",
        "    def generate_out_int(self, n, num_bits_per_symbol):\n",
        "        \"\"\"\"Generates LDPC output interleaver sequence as defined in\n",
        "        Sec 5.4.2.2 in [3GPPTS38212_LDPC]_.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n: int\n",
        "            Desired output sequence length.\n",
        "\n",
        "        num_bits_per_symbol: int\n",
        "            Number of symbols per QAM symbol, i.e., the modulation order.\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "        (perm_seq, perm_seq_inv):\n",
        "            Tuple:\n",
        "\n",
        "        perm_seq: ndarray of length n\n",
        "            Containing the permuted indices.\n",
        "\n",
        "        perm_seq_inv: ndarray of length n\n",
        "            Containing the inverse permuted indices.\n",
        "\n",
        "        Note\n",
        "        ----\n",
        "        The interleaver pattern depends on the modulation order and helps to\n",
        "        reduce dependencies in bit-interleaved coded modulation (BICM) schemes.\n",
        "        \"\"\"\n",
        "        # allow float inputs, but verify that they represent integer\n",
        "        assert(n%1==0), \"n must be int.\"\n",
        "        assert(num_bits_per_symbol%1==0), \"num_bits_per_symbol must be int.\"\n",
        "        n = int(n)\n",
        "        assert(n>0), \"n must be a positive integer.\"\n",
        "        assert(num_bits_per_symbol>0), \\\n",
        "                    \"num_bits_per_symbol must be a positive integer.\"\n",
        "        num_bits_per_symbol = int(num_bits_per_symbol)\n",
        "\n",
        "        assert(n%num_bits_per_symbol==0),\\\n",
        "            \"n must be a multiple of num_bits_per_symbol.\"\n",
        "\n",
        "        # pattern as defined in Sec 5.4.2.2\n",
        "        perm_seq = np.zeros(n, dtype=int)\n",
        "        for j in range(int(n/num_bits_per_symbol)):\n",
        "            for i in range(num_bits_per_symbol):\n",
        "                perm_seq[i + j*num_bits_per_symbol] \\\n",
        "                    = int(i * int(n/num_bits_per_symbol) + j)\n",
        "\n",
        "        perm_seq_inv = np.argsort(perm_seq)\n",
        "\n",
        "        return perm_seq, perm_seq_inv\n",
        "\n",
        "    def _sel_basegraph(self, k, r):\n",
        "        \"\"\"Select basegraph according to [3GPPTS38212_LDPC]_.\"\"\"\n",
        "\n",
        "        if k <= 292:\n",
        "            bg = \"bg2\"\n",
        "        elif k <= 3824 and r <= 0.67:\n",
        "            bg = \"bg2\"\n",
        "        elif r <= 0.25:\n",
        "            bg = \"bg2\"\n",
        "        else:\n",
        "            bg = \"bg1\"\n",
        "\n",
        "        # add for consistency\n",
        "        if bg==\"bg1\" and k>8448:\n",
        "            raise ValueError(\"K is not supported by BG1 (too large).\")\n",
        "\n",
        "        if bg==\"bg2\" and k>3840:\n",
        "            raise ValueError(\n",
        "                f\"K is not supported by BG2 (too large) k ={k}.\")\n",
        "\n",
        "        # if bg==\"bg1\" and r<1/3:\n",
        "        #     raise ValueError(\"Only coderate>1/3 supported for BG1. \\\n",
        "        #     Remark: Repetition coding is currently not supported.\")\n",
        "\n",
        "        # if bg==\"bg2\" and r<1/5:\n",
        "        #     raise ValueError(\"Only coderate>1/5 supported for BG2. \\\n",
        "        #     Remark: Repetition coding is currently not supported.\")\n",
        "\n",
        "        return bg\n",
        "\n",
        "    def _load_basegraph(self, i_ls, bg):\n",
        "        \"\"\"Helper to load basegraph from csv files.\n",
        "\n",
        "        ``i_ls`` is sub_index of the basegraph and fixed during lifting\n",
        "        selection.\n",
        "        \"\"\"\n",
        "\n",
        "        if i_ls > 7:\n",
        "            raise ValueError(\"i_ls too large.\")\n",
        "\n",
        "        if i_ls < 0:\n",
        "            raise ValueError(\"i_ls cannot be negative.\")\n",
        "\n",
        "        # csv files are taken from 38.212 and dimension is explicitly given\n",
        "        if bg==\"bg1\":\n",
        "            bm = np.zeros([46, 68]) - 1 # init matrix with -1 (None positions)\n",
        "        elif bg==\"bg2\":\n",
        "            bm = np.zeros([42, 52]) - 1 # init matrix with -1 (None positions)\n",
        "        else:\n",
        "            raise ValueError(\"Basegraph not supported.\")\n",
        "\n",
        "        # and load the basegraph from csv format in folder \"codes\"\n",
        "        source = files(codes).joinpath(f\"5G_{bg}.csv\")\n",
        "        with as_file(source) as codes.csv:\n",
        "            bg_csv = np.genfromtxt(codes.csv, delimiter=\";\")\n",
        "\n",
        "        # reconstruct BG for given i_ls\n",
        "        r_ind = 0\n",
        "        for r in np.arange(2, bg_csv.shape[0]):\n",
        "            # check for next row index\n",
        "            if not np.isnan(bg_csv[r, 0]):\n",
        "                r_ind = int(bg_csv[r, 0])\n",
        "            c_ind = int(bg_csv[r, 1]) # second column in csv is column index\n",
        "            value = bg_csv[r, i_ls + 2] # i_ls entries start at offset 2\n",
        "            bm[r_ind, c_ind] = value\n",
        "\n",
        "        return bm\n",
        "\n",
        "    def _lift_basegraph(self, bm, z):\n",
        "        \"\"\"Lift basegraph with lifting factor ``z`` and shifted identities as\n",
        "        defined by the entries of ``bm``.\"\"\"\n",
        "\n",
        "        num_nonzero = np.sum(bm>=0) # num of non-neg elements in bm\n",
        "\n",
        "        # init all non-zero row/column indices\n",
        "        r_idx = np.zeros(z*num_nonzero)\n",
        "        c_idx = np.zeros(z*num_nonzero)\n",
        "        data = np.ones(z*num_nonzero)\n",
        "\n",
        "        # row/column indices of identity matrix for lifting\n",
        "        im = np.arange(z)\n",
        "\n",
        "        idx = 0\n",
        "        for r in range(bm.shape[0]):\n",
        "            for c in range(bm.shape[1]):\n",
        "                if bm[r,c]==-1: # -1 is used as all-zero matrix placeholder\n",
        "                    pass #do nothing (sparse)\n",
        "                else:\n",
        "                    # roll matrix by bm[r,c]\n",
        "                    c_roll = np.mod(im+bm[r,c], z)\n",
        "                    # append rolled identity matrix to pcm\n",
        "                    r_idx[idx*z:(idx+1)*z] = r*z + im\n",
        "                    c_idx[idx*z:(idx+1)*z] = c*z + c_roll\n",
        "                    idx += 1\n",
        "\n",
        "        # generate lifted sparse matrix from indices\n",
        "        pcm = sp.sparse.csr_matrix((data,(r_idx, c_idx)),\n",
        "                                   shape=(z*bm.shape[0], z*bm.shape[1]))\n",
        "        return pcm\n",
        "\n",
        "    def _sel_lifting(self, k, bg):\n",
        "        \"\"\"Select lifting as defined in Sec. 5.2.2 in [3GPPTS38212_LDPC]_.\n",
        "\n",
        "        We assume B < K_cb, thus B'= B and C = 1, i.e., no\n",
        "        additional CRC is appended. Thus, K' = B'/C = B and B is our K.\n",
        "\n",
        "        Z is the lifting factor.\n",
        "        i_ls is the set index ranging from 0...7 (specifying the exact bg\n",
        "        selection).\n",
        "        k_b is the number of information bit columns in the basegraph.\n",
        "        \"\"\"\n",
        "        # lifting set according to 38.212 Tab 5.3.2-1\n",
        "        s_val = [[2, 4, 8, 16, 32, 64, 128, 256],\n",
        "                [3, 6, 12, 24, 48, 96, 192, 384],\n",
        "                [5, 10, 20, 40, 80, 160, 320],\n",
        "                [7, 14, 28, 56, 112, 224],\n",
        "                [9, 18, 36, 72, 144, 288],\n",
        "                [11, 22, 44, 88, 176, 352],\n",
        "                [13, 26, 52, 104, 208],\n",
        "                [15, 30, 60, 120, 240]]\n",
        "\n",
        "        if bg == \"bg1\":\n",
        "            k_b = 22\n",
        "        else:\n",
        "            if k > 640:\n",
        "                k_b = 10\n",
        "            elif k > 560:\n",
        "                k_b = 9\n",
        "            elif k > 192:\n",
        "                k_b = 8\n",
        "            else:\n",
        "                k_b = 6\n",
        "\n",
        "        # find the min of Z from Tab. 5.3.2-1 s.t. k_b*Z>=K'\n",
        "        min_val = 100000\n",
        "        z = 0\n",
        "        i_ls = 0\n",
        "        i = -1\n",
        "        for s in s_val:\n",
        "            i += 1\n",
        "            for s1 in s:\n",
        "                x = k_b *s1\n",
        "                if  x >= k:\n",
        "                    # valid solution\n",
        "                    if x < min_val:\n",
        "                        min_val = x\n",
        "                        z = s1\n",
        "                        i_ls = i\n",
        "\n",
        "        # and set K=22*Z for bg1 and K=10Z for bg2\n",
        "        if bg == \"bg1\":\n",
        "            k_b = 22\n",
        "        else:\n",
        "            k_b = 10\n",
        "        # print(z, i_ls, k_b)\n",
        "        return z, i_ls, k_b\n",
        "\n",
        "    def _gen_submat(self, bm, k_b, z, bg):\n",
        "        \"\"\"Split the basegraph into multiple sub-matrices such that efficient\n",
        "        encoding is possible.\n",
        "        \"\"\"\n",
        "        g = 4 # code property (always fixed for 5G)\n",
        "        mb = bm.shape[0] # number of CN rows in basegraph (BG property)\n",
        "\n",
        "        bm_a = bm[0:g, 0:k_b]\n",
        "        bm_b = bm[0:g, k_b:(k_b+g)]\n",
        "        bm_c1 = bm[g:mb, 0:k_b]\n",
        "        bm_c2 = bm[g:mb, k_b:(k_b+g)]\n",
        "        # print(bm_a, bm_b, bm_c1, bm_c2)\n",
        "        # print(np.linalg.norm(bm_a, 'fro'),\n",
        "        #       np.linalg.norm(bm_b, 'fro'),\n",
        "        #       np.linalg.norm(bm_c1, 'fro'),\n",
        "        #       np.linalg.norm(bm_c2, 'fro'))\n",
        "        # H could be sliced immediately (but easier to implement if based on B)\n",
        "        hm_a = self._lift_basegraph(bm_a, z)\n",
        "\n",
        "        # not required for encoding, but helpful for debugging\n",
        "        #hm_b = self._lift_basegraph(bm_b, z)\n",
        "\n",
        "        hm_c1 = self._lift_basegraph(bm_c1, z)\n",
        "        hm_c2 = self._lift_basegraph(bm_c2, z)\n",
        "\n",
        "        hm_b_inv = self._find_hm_b_inv(bm_b, z, bg)\n",
        "        # print(hm_a, hm_b_inv, hm_c1, hm_c2)\n",
        "        # print(np.linalg.norm(hm_a.toarray(), 'fro'),\n",
        "        #       np.linalg.norm(hm_b_inv.toarray(), 'fro'),\n",
        "        #       np.linalg.norm(hm_c1.toarray(), 'fro'),\n",
        "        #       np.linalg.norm(hm_c2.toarray(), 'fro'))\n",
        "        return hm_a, hm_b_inv, hm_c1, hm_c2\n",
        "\n",
        "    def _find_hm_b_inv(self, bm_b, z, bg):\n",
        "        \"\"\" For encoding we need to find the inverse of `hm_b` such that\n",
        "        `hm_b^-1 * hm_b = I`.\n",
        "\n",
        "        Could be done sparse\n",
        "        For BG1 the structure of hm_b is given as (for all values of i_ls)\n",
        "        hm_b =\n",
        "        [P_A I 0 0\n",
        "         P_B I I 0\n",
        "         0 0 I I\n",
        "         P_A 0 0 I]\n",
        "        where P_B and P_A are Shifted identities.\n",
        "\n",
        "        The inverse can be found by solving a linear system of equations\n",
        "        hm_b_inv =\n",
        "        [P_B^-1, P_B^-1, P_B^-1, P_B^-1,\n",
        "         I + P_A*P_B^-1, P_A*P_B^-1, P_A*P_B^-1, P_A*P_B^-1,\n",
        "         P_A*P_B^-1, P_A*P_B^-1, I+P_A*P_B^-1, I+P_A*P_B^-1,\n",
        "         P_A*P_B^-1, P_A*P_B^-1, P_A*P_B^-1, I+P_A*P_B^-1].\n",
        "\n",
        "\n",
        "        For bg2 the structure of hm_b is given as (for all values of i_ls)\n",
        "        hm_b =\n",
        "        [P_A I 0 0\n",
        "         0 I I 0\n",
        "         P_B 0 I I\n",
        "         P_A 0 0 I]\n",
        "        where P_B and P_A are Shifted identities\n",
        "\n",
        "        The inverse can be found by solving a linear system of equations\n",
        "        hm_b_inv =\n",
        "        [P_B^-1, P_B^-1, P_B^-1, P_B^-1,\n",
        "         I + P_A*P_B^-1, P_A*P_B^-1, P_A*P_B^-1, P_A*P_B^-1,\n",
        "         I+P_A*P_B^-1, I+P_A*P_B^-1, P_A*P_B^-1, P_A*P_B^-1,\n",
        "         P_A*P_B^-1, P_A*P_B^-1, P_A*P_B^-1, I+P_A*P_B^-1]\n",
        "\n",
        "        Note: the inverse of B is simply a shifted identity matrix with\n",
        "        negative shift direction.\n",
        "        \"\"\"\n",
        "\n",
        "        # permutation indices\n",
        "        pm_a= int(bm_b[0,0])\n",
        "        if bg==\"bg1\":\n",
        "            pm_b_inv = int(-bm_b[1, 0])\n",
        "        else: # structure of B is slightly different for bg2\n",
        "            pm_b_inv = int(-bm_b[2, 0])\n",
        "\n",
        "        hm_b_inv = np.zeros([4*z, 4*z])\n",
        "\n",
        "        im = np.eye(z)\n",
        "\n",
        "        am = np.roll(im, pm_a, axis=1)\n",
        "        b_inv = np.roll(im, pm_b_inv, axis=1)\n",
        "        ab_inv = np.matmul(am, b_inv)\n",
        "\n",
        "        # row 0\n",
        "        hm_b_inv[0:z, 0:z] = b_inv\n",
        "        hm_b_inv[0:z, z:2*z] = b_inv\n",
        "        hm_b_inv[0:z, 2*z:3*z] = b_inv\n",
        "        hm_b_inv[0:z, 3*z:4*z] = b_inv\n",
        "\n",
        "        # row 1\n",
        "        hm_b_inv[z:2*z, 0:z] = im + ab_inv\n",
        "        hm_b_inv[z:2*z, z:2*z] = ab_inv\n",
        "        hm_b_inv[z:2*z, 2*z:3*z] = ab_inv\n",
        "        hm_b_inv[z:2*z, 3*z:4*z] = ab_inv\n",
        "\n",
        "        # row 2\n",
        "        if bg==\"bg1\":\n",
        "            hm_b_inv[2*z:3*z, 0:z] = ab_inv\n",
        "            hm_b_inv[2*z:3*z, z:2*z] = ab_inv\n",
        "            hm_b_inv[2*z:3*z, 2*z:3*z] = im + ab_inv\n",
        "            hm_b_inv[2*z:3*z, 3*z:4*z] = im + ab_inv\n",
        "        else: # for bg2 the structure is slightly different\n",
        "            hm_b_inv[2*z:3*z, 0:z] = im + ab_inv\n",
        "            hm_b_inv[2*z:3*z, z:2*z] = im + ab_inv\n",
        "            hm_b_inv[2*z:3*z, 2*z:3*z] = ab_inv\n",
        "            hm_b_inv[2*z:3*z, 3*z:4*z] = ab_inv\n",
        "\n",
        "        # row 3\n",
        "        hm_b_inv[3*z:4*z, 0:z] = ab_inv\n",
        "        hm_b_inv[3*z:4*z, z:2*z] = ab_inv\n",
        "        hm_b_inv[3*z:4*z, 2*z:3*z] = ab_inv\n",
        "        hm_b_inv[3*z:4*z, 3*z:4*z] = im + ab_inv\n",
        "\n",
        "        # return results as sparse matrix\n",
        "        return sp.sparse.csr_matrix(hm_b_inv)\n",
        "\n",
        "    def _mat_to_ind(self, mat):\n",
        "        \"\"\"Helper to transform matrix into index representation for\n",
        "        tf.gather. An index pointing to the `last_ind+1` is used for non-existing edges due to irregular degrees.\"\"\"\n",
        "        m = mat.shape[0]\n",
        "        n = mat.shape[1]\n",
        "\n",
        "        # transpose mat for sorted column format\n",
        "        c_idx, r_idx, _ = sp.sparse.find(mat.transpose())\n",
        "\n",
        "        # sort indices explicitly, as scipy.sparse.find changed from column to\n",
        "        # row sorting in scipy>=1.11\n",
        "        idx = np.argsort(r_idx)\n",
        "        c_idx = c_idx[idx]\n",
        "        r_idx = r_idx[idx]\n",
        "\n",
        "        # find max number of no-zero entries\n",
        "        n_max = np.max(mat.getnnz(axis=1))\n",
        "\n",
        "        # init index array with n (pointer to last_ind+1, will be a default\n",
        "        # value)\n",
        "        gat_idx = np.zeros([m, n_max]) + n\n",
        "\n",
        "        r_val = -1\n",
        "        c_val = 0\n",
        "        for idx in range(len(c_idx)):\n",
        "            # check if same row or if a new row starts\n",
        "            if r_idx[idx] != r_val:\n",
        "                r_val = r_idx[idx]\n",
        "                c_val = 0\n",
        "            gat_idx[r_val, c_val] = c_idx[idx]\n",
        "            c_val += 1\n",
        "\n",
        "        gat_idx = tf.cast(tf.constant(gat_idx), tf.int32)\n",
        "        return gat_idx\n",
        "\n",
        "    def _matmul_gather(self, mat, vec):\n",
        "        \"\"\"Implements a fast sparse matmul via gather function.\"\"\"\n",
        "\n",
        "        # add 0 entry for gather-reduce_sum operation\n",
        "        # (otherwise ragged Tensors are required)\n",
        "        bs = tf.shape(vec)[0]\n",
        "        vec = tf.concat([vec, tf.zeros([bs, 1], dtype=self.dtype)], 1)\n",
        "\n",
        "        retval = tf.gather(vec, mat, batch_dims=0, axis=1)\n",
        "        retval = tf.reduce_sum(retval, axis=-1)\n",
        "\n",
        "        return retval\n",
        "\n",
        "    def _encode_fast(self, s):\n",
        "        \"\"\"Main encoding function based on gathering function.\"\"\"\n",
        "        s_2 = s\n",
        "        print('s_2[:, 2*self._z:]', poly_hash(s_2[:, 2*self._z:]), s_2[:, 2*self._z:].shape)\n",
        "\n",
        "        s = tf.where(s == -1, tf.zeros_like(s), s)\n",
        "\n",
        "        p_a = self._matmul_gather(self._pcm_a_ind, s)\n",
        "        p_a = self._matmul_gather(self._pcm_b_inv_ind, p_a)\n",
        "        # calc second part of parity bits p_b\n",
        "        # second parities are given by C_1*s' + C_2*p_a' + p_b' = 0\n",
        "        p_b_1 = self._matmul_gather(self._pcm_c1_ind, s)\n",
        "        p_b_2 = self._matmul_gather(self._pcm_c2_ind, p_a)\n",
        "        p_b = p_b_1 + p_b_2\n",
        "\n",
        "        w = tf.concat([p_a, p_b], 1)\n",
        "        w = tf.cast(w, tf.uint8)\n",
        "        w = tf.bitwise.bitwise_and(w, tf.constant(1, tf.uint8))\n",
        "        w = tf.cast(w, self.dtype)\n",
        "        print('w', poly_hash(w), w.shape)\n",
        "\n",
        "        c = tf.concat([s_2, w], 1)\n",
        "        print('c[:, 2*self._z:]', poly_hash(c[:, 2*self._z:]), c[:, 2*self._z:].shape)\n",
        "        return c\n",
        "        c = tf.concat([s_2, p_a, p_b], 1)\n",
        "        # faster implementation of mod-2 operation c = tf.math.mod(c, 2)\n",
        "        c_uint8 = tf.cast(c, tf.uint8)\n",
        "        c_bin = tf.bitwise.bitwise_and(c_uint8, tf.constant(1, tf.uint8))\n",
        "        c = tf.cast(c_bin, self.dtype)\n",
        "        c = tf.expand_dims(c, axis=-1) # returns nx1 vector\n",
        "        return c\n",
        "\n",
        "    #########################\n",
        "    # Keras layer functions\n",
        "    #########################\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"\"Build layer.\"\"\"\n",
        "        # check if k and input shape match\n",
        "        assert (input_shape[-1]==self._k), \"Last dimension must be of length k.\"\n",
        "        assert (len(input_shape)>=2), \"Rank of input must be at least 2.\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"5G LDPC encoding function including rate-matching.\n",
        "\n",
        "        This function returns the encoded codewords as specified by the 3GPP NR Initiative [3GPPTS38212_LDPC]_ including puncturing and shortening.\n",
        "\n",
        "        Args:\n",
        "            inputs (tf.float32): Tensor of shape `[...,k]` containing the\n",
        "                information bits to be encoded.\n",
        "\n",
        "        Returns:\n",
        "            `tf.float32`: Tensor of shape `[...,n]`.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If ``inputs`` contains other values than `0` or `1`.\n",
        "\n",
        "            InvalidArgumentError: When rank(``inputs``)<2.\n",
        "\n",
        "            InvalidArgumentError: When shape of last dim is not ``k``.\n",
        "        \"\"\"\n",
        "\n",
        "        tf.debugging.assert_type(inputs, self.dtype, \"Invalid input dtype.\")\n",
        "\n",
        "        # Reshape inputs to [...,k]\n",
        "        input_shape = inputs.get_shape().as_list()\n",
        "        new_shape = [-1, input_shape[-1]]\n",
        "        u = tf.reshape(inputs, new_shape)\n",
        "        print('u', poly_hash(u), u.shape)\n",
        "        # assert if u is non binary\n",
        "        if self._check_input:\n",
        "            tf.debugging.assert_equal(\n",
        "                tf.reduce_min(\n",
        "                    tf.cast(\n",
        "                        tf.logical_or(\n",
        "                            tf.equal(u, tf.constant(0, self.dtype)),\n",
        "                            tf.equal(u, tf.constant(1, self.dtype)),\n",
        "                            ),\n",
        "                        self.dtype)),\n",
        "                tf.constant(1, self.dtype),\n",
        "                \"Input must be binary.\")\n",
        "            # input datatype consistency should be only evaluated once\n",
        "            self._check_input = False\n",
        "\n",
        "        batch_size = tf.shape(u)[0]\n",
        "\n",
        "        # add \"filler\" bits to last positions to match info bit length k_ldpc\n",
        "        u_fill = tf.concat([u,\n",
        "                    -tf.ones([batch_size, self._k_ldpc-self._k], self.dtype)],\n",
        "                            1)\n",
        "        print('u_fill', poly_hash(u_fill), u_fill.shape)\n",
        "        # use optimized encoding based on tf.gather\n",
        "        c = self._encode_fast(u_fill)\n",
        "        c = tf.reshape(c, [batch_size, self._n_ldpc]) # remove last dim\n",
        "        print('c', poly_hash(c), c.shape)\n",
        "        print('c[:, 2*self._z:]', poly_hash(c[:, 2*self._z:]), c[:, 2*self._z:].shape)\n",
        "\n",
        "      \n",
        "        print('z', self._z, 'n', self.n, 'n_lpdc', self._n_ldpc, 'k', self._k, 'k_lpdc', self._k_ldpc)\n",
        "\n",
        "        shorten_size = self._n+self._k_ldpc-self._k\n",
        "        print('shorten_size', shorten_size, 'shorten_size-self._k_ldpc+2*self._z', shorten_size-self._k_ldpc+2*self._z, 'self._k-2*self._z', self._k-2*self._z)\n",
        "        c_short = tf.slice(c, [0, 2*self._z], [batch_size,  shorten_size])\n",
        "        print('c_short', poly_hash(c_short), c_short.shape)\n",
        "        c_no_filler1 = tf.slice(c_short, [0, 0], [batch_size, self._k-2*self._z])\n",
        "        c_no_filler2 = tf.slice(c_short,\n",
        "                               [0, self._k_ldpc-2*self._z],\n",
        "                               [batch_size, shorten_size - self._k_ldpc+2*self._z])\n",
        "        \n",
        "        c_no_filler = tf.concat([c_no_filler1, c_no_filler2], 1)\n",
        "\n",
        "        print('c_no_filler', poly_hash(c_no_filler), c_no_filler.shape)\n",
        "\n",
        "        # remove filler bits at pos (k, k_ldpc)\n",
        "        c_no_filler1 = tf.slice(c, [0, 0], [batch_size, self._k])\n",
        "        c_no_filler2 = tf.slice(c,\n",
        "                               [0, self._k_ldpc],\n",
        "                               [batch_size, self._n_ldpc-self._k_ldpc])\n",
        "        \n",
        "        print('c_no_filler1', poly_hash(c_no_filler1), c_no_filler1.shape)\n",
        "        print('c_no_filler2', poly_hash(c_no_filler2), c_no_filler2.shape)\n",
        "        c_no_filler = tf.concat([c_no_filler1, c_no_filler2], 1)\n",
        "        print('c_no_filler', poly_hash(c_no_filler), c_no_filler.shape)\n",
        "        # shorten the first 2*Z positions and end after n bits\n",
        "        # (remaining parity bits can be used for IR-HARQ)\n",
        "        \n",
        "        c_short = tf.slice(c_no_filler, [0, 2*self._z], [batch_size, self.n])\n",
        "        print('c_short', poly_hash(c_short), c_short.shape)\n",
        "        # incremental redundancy could be generated by accessing the last bits\n",
        "        # print('c_short',c_short.shape, poly_hash(c_short, [1]))\n",
        "        # if num_bits_per_symbol is provided, apply output interleaver as\n",
        "        # specified in Sec. 5.4.2.2 in 38.212\n",
        "        if self._num_bits_per_symbol is not None:\n",
        "            c_short = tf.gather(c_short, self._out_int, axis=-1)\n",
        "        print('c_short', poly_hash(c_short), c_short.shape)\n",
        "        # Reshape c_short so that it matches the original input dimensions\n",
        "        output_shape = input_shape[0:-1] + [self.n]\n",
        "        output_shape[0] = -1\n",
        "        c_reshaped = tf.reshape(c_short, output_shape)\n",
        "\n",
        "        return tf.cast(c_reshaped, self._dtype)\n",
        "\n",
        "\n",
        "###########################################################\n",
        "# Deprecated aliases that will not be included in the next\n",
        "# major release\n",
        "###########################################################\n",
        "\n",
        "def AllZeroEncoder(k,\n",
        "                   n,\n",
        "                   dtype=tf.float32,\n",
        "                   **kwargs):\n",
        "    print(\"Warning: The alias fec.ldpc.AllZeroEncoder will not be included in \"\\\n",
        "          \"Sionna 1.0. Please use sionna.fec.linear.AllZeroEncoder instead.\")\n",
        "    return AllZeroEncoder_new(k=k,\n",
        "                              n=n,\n",
        "                              dtype=dtype,\n",
        "                              **kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sionna.fec.crc import CRCEncoder\n",
        "from sionna.fec.scrambling import TB5GScrambler\n",
        "from sionna.nr.utils import calculate_tb_size\n",
        "\n",
        "class TBEncoder(Layer):\n",
        "    # pylint: disable=line-too-long\n",
        "    r\"\"\"TBEncoder(target_tb_size,num_coded_bits,target_coderate,num_bits_per_symbol,num_layers=1,n_rnti=1,n_id=1,channel_type=\"PUSCH\",codeword_index=0,use_scrambler=True,verbose=False,output_dtype=tf.float32,, **kwargs)\n",
        "    5G NR transport block (TB) encoder as defined in TS 38.214\n",
        "    [3GPP38214]_ and TS 38.211 [3GPP38211]_\n",
        "\n",
        "    The transport block (TB) encoder takes as input a `transport block` of\n",
        "    information bits and generates a sequence of codewords for transmission.\n",
        "    For this, the information bit sequence is segmented into multiple codewords,\n",
        "    protected by additional CRC checks and FEC encoded. Further, interleaving\n",
        "    and scrambling is applied before a codeword concatenation generates the\n",
        "    final bit sequence. Fig. 1 provides an overview of the TB encoding\n",
        "    procedure and we refer the interested reader to [3GPP38214]_ and\n",
        "    [3GPP38211]_ for further details.\n",
        "\n",
        "    ..  figure:: ../figures/tb_encoding.png\n",
        "\n",
        "        Fig. 1: Overview TB encoding (CB CRC does not always apply).\n",
        "\n",
        "    If ``n_rnti`` and ``n_id`` are given as list, the TBEncoder encodes\n",
        "    `num_tx = len(` ``n_rnti`` `)` parallel input streams with different\n",
        "    scrambling sequences per user.\n",
        "\n",
        "    The class inherits from the Keras layer class and can be used as layer in a\n",
        "    Keras model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        target_tb_size: int\n",
        "            Target transport block size, i.e., how many information bits are\n",
        "            encoded into the TB. Note that the effective TB size can be\n",
        "            slightly different due to quantization. If required, zero padding\n",
        "            is internally applied.\n",
        "\n",
        "        num_coded_bits: int\n",
        "            Number of coded bits after TB encoding.\n",
        "\n",
        "        target_coderate : float\n",
        "            Target coderate.\n",
        "\n",
        "        num_bits_per_symbol: int\n",
        "            Modulation order, i.e., number of bits per QAM symbol.\n",
        "\n",
        "        num_layers: int, 1 (default) | [1,...,8]\n",
        "            Number of transmission layers.\n",
        "\n",
        "        n_rnti: int or list of ints, 1 (default) | [0,...,65335]\n",
        "            RNTI identifier provided by higher layer. Defaults to 1 and must be\n",
        "            in range `[0, 65335]`. Defines a part of the random seed of the\n",
        "            scrambler. If provided as list, every list entry defines the RNTI\n",
        "            of an independent input stream.\n",
        "\n",
        "        n_id: int or list of ints, 1 (default) | [0,...,1023]\n",
        "            Data scrambling ID :math:`n_\\text{ID}` related to cell id and\n",
        "            provided by higher layer.\n",
        "            Defaults to 1 and must be in range `[0, 1023]`. If provided as\n",
        "            list, every list entry defines the scrambling id of an independent\n",
        "            input stream.\n",
        "\n",
        "        channel_type: str, \"PUSCH\" (default) | \"PDSCH\"\n",
        "            Can be either \"PUSCH\" or \"PDSCH\".\n",
        "\n",
        "        codeword_index: int, 0 (default) | 1\n",
        "            Scrambler can be configured for two codeword transmission.\n",
        "            ``codeword_index`` can be either 0 or 1. Must be 0 for\n",
        "            ``channel_type`` = \"PUSCH\".\n",
        "\n",
        "        use_scrambler: bool, True (default)\n",
        "            If False, no data scrambling is applied (non standard-compliant).\n",
        "\n",
        "        verbose: bool, False (default)\n",
        "            If `True`, additional parameters are printed during initialization.\n",
        "\n",
        "        dtype: tf.float32 (default)\n",
        "            Defines the datatype for internal calculations and the output dtype.\n",
        "\n",
        "    Input\n",
        "    -----\n",
        "        inputs: [...,target_tb_size] or [...,num_tx,target_tb_size], tf.float\n",
        "            2+D tensor containing the information bits to be encoded. If\n",
        "            ``n_rnti`` and ``n_id`` are a list of size `num_tx`, the input must\n",
        "            be of shape `[...,num_tx,target_tb_size]`.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        : [...,num_coded_bits], tf.float\n",
        "            2+D tensor containing the sequence of the encoded codeword bits of\n",
        "            the transport block.\n",
        "\n",
        "    Note\n",
        "    ----\n",
        "    The parameters ``tb_size`` and ``num_coded_bits`` can be derived by the\n",
        "    :meth:`~sionna.nr.calculate_tb_size` function or\n",
        "    by accessing the corresponding :class:`~sionna.nr.PUSCHConfig` attributes.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 target_tb_size,\n",
        "                 num_coded_bits,\n",
        "                 target_coderate,\n",
        "                 num_bits_per_symbol,\n",
        "                 num_layers=1,\n",
        "                 n_rnti=1,\n",
        "                 n_id=1,\n",
        "                 channel_type=\"PUSCH\",\n",
        "                 codeword_index=0,\n",
        "                 use_scrambler=True,\n",
        "                 verbose=False,\n",
        "                 output_dtype=tf.float32,\n",
        "                 **kwargs):\n",
        "\n",
        "        super().__init__(dtype=output_dtype, **kwargs)\n",
        "\n",
        "        assert isinstance(use_scrambler, bool), \\\n",
        "                                \"use_scrambler must be bool.\"\n",
        "        self._use_scrambler = use_scrambler\n",
        "        assert isinstance(verbose, bool), \\\n",
        "                                \"verbose must be bool.\"\n",
        "        self._verbose = verbose\n",
        "\n",
        "        # check input for consistency\n",
        "        assert channel_type in (\"PDSCH\", \"PUSCH\"), \\\n",
        "                                \"Unsupported channel_type.\"\n",
        "        self._channel_type = channel_type\n",
        "\n",
        "        assert(target_tb_size%1==0), \"target_tb_size must be int.\"\n",
        "        self._target_tb_size = int(target_tb_size)\n",
        "\n",
        "        assert(num_coded_bits%1==0), \"num_coded_bits must be int.\"\n",
        "        self._num_coded_bits = int(num_coded_bits)\n",
        "\n",
        "        assert(0.<target_coderate <= 948/1024), \\\n",
        "                    \"target_coderate must be in range(0,0.925).\"\n",
        "        self._target_coderate = target_coderate\n",
        "\n",
        "        assert(num_bits_per_symbol%1==0), \"num_bits_per_symbol must be int.\"\n",
        "        self._num_bits_per_symbol = int(num_bits_per_symbol)\n",
        "\n",
        "        assert(num_layers%1==0), \"num_layers must be int.\"\n",
        "        self._num_layers = int(num_layers)\n",
        "\n",
        "        if channel_type==\"PDSCH\":\n",
        "            assert(codeword_index in (0,1)), \"codeword_index must be 0 or 1.\"\n",
        "        else:\n",
        "            assert codeword_index==0, 'codeword_index must be 0 for \"PUSCH\".'\n",
        "        self._codeword_index = int(codeword_index)\n",
        "\n",
        "        if isinstance(n_rnti, (list, tuple)):\n",
        "            assert isinstance(n_id, (list, tuple)), \"n_id must be also a list.\"\n",
        "            assert (len(n_rnti)==len(n_id)), \\\n",
        "                                \"n_id and n_rnti must be of same length.\"\n",
        "            self._n_rnti = n_rnti\n",
        "            self._n_id = n_id\n",
        "        else:\n",
        "            self._n_rnti = [n_rnti]\n",
        "            self._n_id = [n_id]\n",
        "\n",
        "        for idx, n in enumerate(self._n_rnti):\n",
        "            assert(n%1==0), \"n_rnti must be int.\"\n",
        "            self._n_rnti[idx] = int(n)\n",
        "        for idx, n in enumerate(self._n_id):\n",
        "            assert(n%1==0), \"n_id must be int.\"\n",
        "            self._n_id[idx] = int(n)\n",
        "\n",
        "        self._num_tx = len(self._n_id)\n",
        "\n",
        "        tbconfig = calculate_tb_size(target_tb_size=self._target_tb_size,\n",
        "                                     num_coded_bits=self._num_coded_bits,\n",
        "                                     target_coderate=self._target_coderate,\n",
        "                                     modulation_order=self._num_bits_per_symbol,\n",
        "                                     num_layers=self._num_layers,\n",
        "                                     verbose=verbose)\n",
        "        self._tb_size = tbconfig[0]\n",
        "        self._cb_size = tbconfig[1]\n",
        "        self._num_cbs = tbconfig[2]\n",
        "        self._cw_lengths = tbconfig[3]\n",
        "        self._tb_crc_length = tbconfig[4]\n",
        "        self._cb_crc_length = tbconfig[5]\n",
        "\n",
        "        assert self._tb_size <= self._tb_crc_length + np.sum(self._cw_lengths),\\\n",
        "            \"Invalid TB parameters.\"\n",
        "\n",
        "        # due to quantization, the tb_size can slightly differ from the\n",
        "        # target tb_size.\n",
        "        self._k_padding = self._tb_size - self._target_tb_size\n",
        "        if self._tb_size != self._target_tb_size:\n",
        "            print(f\"Note: actual tb_size={self._tb_size} is slightly \"\\\n",
        "                  f\"different than requested \" \\\n",
        "                  f\"target_tb_size={self._target_tb_size} due to \"\\\n",
        "                  f\"quantization. Internal zero padding will be applied.\")\n",
        "\n",
        "        # calculate effective coderate (incl. CRC)\n",
        "        self._coderate = self._tb_size / self._num_coded_bits\n",
        "\n",
        "        # Remark: CRC16 is only used for k<3824 (otherwise CRC24)\n",
        "        if self._tb_crc_length==16:\n",
        "            self._tb_crc_encoder = CRCEncoder(\"CRC16\")\n",
        "        else:\n",
        "            # CRC24A as defined in 7.2.1\n",
        "            self._tb_crc_encoder = CRCEncoder(\"CRC24A\")\n",
        "\n",
        "        # CB CRC only if more than one CB is used\n",
        "        if self._cb_crc_length==24:\n",
        "            self._cb_crc_encoder = CRCEncoder(\"CRC24B\")\n",
        "        else:\n",
        "            self._cb_crc_encoder = None\n",
        "\n",
        "        # scrambler can be deactivated (non-standard compliant)\n",
        "        if self._use_scrambler:\n",
        "            self._scrambler = TB5GScrambler(n_rnti=self._n_rnti,\n",
        "                                            n_id=self._n_id,\n",
        "                                            binary=True,\n",
        "                                            channel_type=channel_type,\n",
        "                                            codeword_index=codeword_index,\n",
        "                                            dtype=tf.float32,)\n",
        "        else: # required for TBDecoder\n",
        "            self._scrambler = None\n",
        "\n",
        "        # ---- Init LDPC encoder ----\n",
        "        # remark: as the codeword length can be (slightly) different\n",
        "        # within a TB due to rounding, we initialize the encoder\n",
        "        # with the max length and apply puncturing if required.\n",
        "        # Thus, also the output interleaver cannot be applied in the encoder.\n",
        "        # The procedure is defined in in 5.4.2.1 38.212\n",
        "        self._encoder = LDPC5GEncoder(self._cb_size,\n",
        "                                      np.max(self._cw_lengths),\n",
        "                                      num_bits_per_symbol=1) #deact. interleaver\n",
        "\n",
        "        # ---- Init interleaver ----\n",
        "        # remark: explicit interleaver is required as the rate matching from\n",
        "        # Sec. 5.4.2.1 38.212 could otherwise not be applied here\n",
        "        perm_seq_short, _ = self._encoder.generate_out_int(\n",
        "                                            np.min(self._cw_lengths),\n",
        "                                            num_bits_per_symbol)\n",
        "        perm_seq_long, _ = self._encoder.generate_out_int(\n",
        "                                            np.max(self._cw_lengths),\n",
        "                                            num_bits_per_symbol)\n",
        "\n",
        "        perm_seq = []\n",
        "        perm_seq_punc = []\n",
        "\n",
        "        # define one big interleaver that moves the punctured positions to the\n",
        "        # end of the TB\n",
        "        payload_bit_pos = 0 # points to current pos of payload bits\n",
        "\n",
        "        for l in self._cw_lengths:\n",
        "            if np.min(self._cw_lengths)==l:\n",
        "                perm_seq = np.concatenate([perm_seq,\n",
        "                                           perm_seq_short + payload_bit_pos])\n",
        "                # move unused bit positions to the end of TB\n",
        "                # this simplifies the inverse permutation\n",
        "                r = np.arange(payload_bit_pos+np.min(self._cw_lengths),\n",
        "                              payload_bit_pos+np.max(self._cw_lengths))\n",
        "                perm_seq_punc = np.concatenate([perm_seq_punc, r])\n",
        "\n",
        "                # update pointer\n",
        "                payload_bit_pos += np.max(self._cw_lengths)\n",
        "            elif np.max(self._cw_lengths)==l:\n",
        "                perm_seq = np.concatenate([perm_seq,\n",
        "                                           perm_seq_long + payload_bit_pos])\n",
        "                # update pointer\n",
        "                payload_bit_pos += l\n",
        "            else:\n",
        "                raise ValueError(\"Invalid cw_lengths.\")\n",
        "\n",
        "        # add punctured positions to end of sequence (only relevant for\n",
        "        # deinterleaving)\n",
        "        perm_seq = np.concatenate([perm_seq, perm_seq_punc])\n",
        "\n",
        "        self._output_perm = tf.constant(perm_seq, tf.int32)\n",
        "        self._output_perm_inv = tf.argsort(perm_seq, axis=-1)\n",
        "\n",
        "    #########################################\n",
        "    # Public methods and properties\n",
        "    #########################################\n",
        "\n",
        "\n",
        "    @property\n",
        "    def tb_size(self):\n",
        "        r\"\"\"Effective number of information bits per TB.\n",
        "        Note that (if required) internal zero padding can be applied to match\n",
        "        the request exact ``target_tb_size``.\"\"\"\n",
        "        return self._tb_size\n",
        "\n",
        "    @property\n",
        "    def k(self):\n",
        "        r\"\"\"Number of input information bits. Equals `tb_size` except for zero\n",
        "        padding of the last positions if the ``target_tb_size`` is quantized.\"\"\"\n",
        "        return self._target_tb_size\n",
        "\n",
        "    @property\n",
        "    def k_padding(self):\n",
        "        \"\"\"Number of zero padded bits at the end of the TB.\"\"\"\n",
        "        return self._k_padding\n",
        "\n",
        "    @property\n",
        "    def n(self):\n",
        "        \"Total number of output bits.\"\n",
        "        return self._num_coded_bits\n",
        "\n",
        "    @property\n",
        "    def num_cbs(self):\n",
        "        \"Number code blocks.\"\n",
        "        return self._num_cbs\n",
        "\n",
        "    @property\n",
        "    def coderate(self):\n",
        "        \"\"\"Effective coderate of the TB after rate-matching including overhead\n",
        "        for the CRC.\"\"\"\n",
        "        return self._coderate\n",
        "\n",
        "    @property\n",
        "    def ldpc_encoder(self):\n",
        "        \"\"\"LDPC encoder used for TB encoding.\"\"\"\n",
        "        return self._encoder\n",
        "\n",
        "    @property\n",
        "    def scrambler(self):\n",
        "        \"\"\"Scrambler used for TB scrambling. `None` if no scrambler is used.\"\"\"\n",
        "        return self._scrambler\n",
        "\n",
        "    @property\n",
        "    def tb_crc_encoder(self):\n",
        "        \"\"\"TB CRC encoder\"\"\"\n",
        "        return self._tb_crc_encoder\n",
        "\n",
        "    @property\n",
        "    def cb_crc_encoder(self):\n",
        "        \"\"\"CB CRC encoder. `None` if no CB CRC is applied.\"\"\"\n",
        "        return self._cb_crc_encoder\n",
        "\n",
        "    @property\n",
        "    def num_tx(self):\n",
        "        \"\"\"Number of independent streams\"\"\"\n",
        "        return self._num_tx\n",
        "\n",
        "    @property\n",
        "    def cw_lengths(self):\n",
        "        r\"\"\"Each list element defines the codeword length of each of the\n",
        "        codewords after LDPC encoding and rate-matching. The total number of\n",
        "        coded bits is :math:`\\sum` `cw_lengths`.\"\"\"\n",
        "        return self._cw_lengths\n",
        "\n",
        "    @property\n",
        "    def output_perm_inv(self):\n",
        "        r\"\"\"Inverse interleaver pattern for output bit interleaver.\"\"\"\n",
        "        return self._output_perm_inv\n",
        "\n",
        "    #########################\n",
        "    # Keras layer functions\n",
        "    #########################\n",
        "\n",
        "    def build(self, input_shapes):\n",
        "        \"\"\"Test input shapes for consistency.\"\"\"\n",
        "\n",
        "        assert input_shapes[-1]==self.k, \\\n",
        "            f\"Invalid input shape. Expected TB length is {self.k}.\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Apply transport block encoding procedure.\"\"\"\n",
        "\n",
        "        # store shapes\n",
        "        input_shape = inputs.shape.as_list()\n",
        "        u = tf.cast(inputs, tf.float32)\n",
        "        print('u', poly_hash(u), u.shape)\n",
        "        # apply zero padding if tb_size is slightly different to target_tb_size\n",
        "        if self._k_padding>0:\n",
        "            s = tf.shape(u)\n",
        "            s = tf.concat((s[:-1], [self._k_padding]), axis=0)\n",
        "            u = tf.concat((u, tf.zeros(s, u.dtype)), axis=-1)\n",
        "\n",
        "        # apply TB CRC\n",
        "        u_crc = self._tb_crc_encoder(u)\n",
        "        print('u_crc', poly_hash(u_crc), u_crc.shape)\n",
        "        # CB segmentation\n",
        "        u_cb = tf.reshape(u_crc,\n",
        "                          (-1, self._num_tx, self._num_cbs,\n",
        "                          self._cb_size-self._cb_crc_length))\n",
        "        print('u_cb', poly_hash(u_cb), u_cb.shape)\n",
        "        # if relevant apply CB CRC\n",
        "        if self._cb_crc_length==24:\n",
        "            u_cb_crc = self._cb_crc_encoder(u_cb)\n",
        "        else:\n",
        "            u_cb_crc = u_cb # no CRC applied if only one CB exists\n",
        "        print('u_cb_crc', poly_hash(u_cb_crc), u_cb_crc.shape)\n",
        "        c_cb = self._encoder(u_cb_crc)\n",
        "        print('c_cb', poly_hash(c_cb), u_cb.shape)\n",
        "        # CB concatenation\n",
        "        c = tf.reshape(c_cb,\n",
        "                       (-1, self._num_tx,\n",
        "                       self._num_cbs*np.max(self._cw_lengths)))\n",
        "\n",
        "        # apply interleaver (done after CB concatenation)\n",
        "        print('c', poly_hash(c), c.shape)\n",
        "        print('output_perm', poly_hash(self._output_perm), self._output_perm.shape)\n",
        "        c = tf.gather(c, self._output_perm, axis=-1)\n",
        "        print('c', poly_hash(c), c.shape, np.sum(self._cw_lengths), self._n_rnti, self._n_id)\n",
        "        # puncture last bits\n",
        "        c = c[:, :, :np.sum(self._cw_lengths)]\n",
        "        print('c', poly_hash(c), c.shape, np.sum(self._cw_lengths), self._n_rnti, self._n_id)\n",
        "        # scrambler\n",
        "        if self._use_scrambler:\n",
        "            c_scr = self._scrambler(c)\n",
        "        else: # disable scrambler (non-standard compliant)\n",
        "            c_scr = c\n",
        "        print('c_scr', poly_hash(c_scr[:,:,:]), c_scr.shape)\n",
        "        # cast to output dtype\n",
        "        c_scr = tf.cast(c_scr, self.dtype)\n",
        "\n",
        "        # ensure output shapes\n",
        "        output_shape = input_shape\n",
        "        output_shape[0] = -1\n",
        "        output_shape[-1] = np.sum(self._cw_lengths)\n",
        "        c_tb = tf.reshape(c_scr, output_shape)\n",
        "        print('c_tb', poly_hash(c_tb), c_tb.shape)\n",
        "        return c_tb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MySimulator():\n",
        "    def __init__(self, pusch_config: MyPUSCHConfig):\n",
        "\n",
        "        tb_size = pusch_config.tb_size\n",
        "        num_coded_bits = pusch_config.num_coded_bits\n",
        "        target_coderate = pusch_config.tb.target_coderate\n",
        "        num_bits_per_symbol = pusch_config.tb.num_bits_per_symbol\n",
        "\n",
        "        num_layers = pusch_config.num_layers\n",
        "        n_rnti = pusch_config.n_rnti\n",
        "        n_id = pusch_config.tb.n_id\n",
        "\n",
        "        self.Binary_Source = BinarySource(dtype=tf.float32)\n",
        "        self.TB_Encoder = TBEncoder(target_tb_size=tb_size,\n",
        "                            num_coded_bits=num_coded_bits,\n",
        "                            target_coderate=target_coderate,\n",
        "                            num_bits_per_symbol=num_bits_per_symbol,\n",
        "                            num_layers=num_layers,\n",
        "                            n_rnti=n_rnti,\n",
        "                            n_id=n_id,\n",
        "                            channel_type=\"PUSCH\",\n",
        "                            codeword_index=0,\n",
        "                            use_scrambler=True,\n",
        "                            verbose=False,\n",
        "                            output_dtype=tf.float32)\n",
        "        \n",
        "        self.Constellation_Mapper = Mapper(\"qam\", num_bits_per_symbol, dtype=tf.complex64)\n",
        "\n",
        "        self.Layer_Mapper = LayerMapper(num_layers=num_layers, dtype=tf.complex64)\n",
        "    \n",
        "        self.Pilot_Pattern = PUSCHPilotPattern([pusch_config], dtype=tf.complex64)\n",
        "\n",
        "        num_subcarriers = pusch_config.num_subcarriers\n",
        "        subcarrier_spacing = pusch_config.carrier.subcarrier_spacing*1e3\n",
        "        fft_size = num_subcarriers\n",
        "        cp_length = min(num_subcarriers, 288)\n",
        "        guard_subcarriers = (0,0)\n",
        "        # Define the resource grid.\n",
        "        resource_grid = ResourceGrid(\n",
        "            num_ofdm_symbols=14,\n",
        "            fft_size=fft_size,\n",
        "            subcarrier_spacing=subcarrier_spacing,\n",
        "            num_tx=NUM_TX,\n",
        "            num_streams_per_tx=NUM_STREAMS_PER_TX,\n",
        "            cyclic_prefix_length=cp_length,\n",
        "            num_guard_carriers=guard_subcarriers,\n",
        "            dc_null=False,\n",
        "            pilot_pattern=self.Pilot_Pattern,\n",
        "            dtype=tf.complex64\n",
        "        )\n",
        "\n",
        "        self.Resource_Grid_Mapper = ResourceGridMapper(resource_grid, dtype=tf.complex64)        \n",
        "        \n",
        "        self.AWGN = AWGN()\n",
        "\n",
        " \n",
        "        self.Channel_Estimator = PUSCHLSChannelEstimator(\n",
        "                        resource_grid,\n",
        "                        pusch_config.dmrs.length,\n",
        "                        pusch_config.dmrs.additional_position,\n",
        "                        pusch_config.dmrs.num_cdm_groups_without_data,\n",
        "                        interpolation_type='nn',\n",
        "                        dtype=tf.complex64)\n",
        "\n",
        "        rxtx_association = np.ones([NUM_RX, NUM_TX], bool)\n",
        "        stream_management = StreamManagement(rxtx_association, pusch_config.num_layers)\n",
        "        self.Mimo_Detector = LinearDetector(\"lmmse\", \"bit\", \"maxlog\", resource_grid, stream_management,\n",
        "                                    \"qam\", pusch_config.tb.num_bits_per_symbol, dtype=tf.complex64)\n",
        "        \n",
        "        self.Equalizer = LinearDetector(\"lmmse\", \"symbol\", \"maxlog\", resource_grid, stream_management,\n",
        "                                    \"qam\", pusch_config.tb.num_bits_per_symbol, dtype=tf.complex64)\n",
        "\n",
        "        self.Layer_Demapper = LayerDemapper(self.Layer_Mapper, num_bits_per_symbol=num_bits_per_symbol)\n",
        "        # self.TB_Decoder = TBDecoder(self.TB_Encoder, output_dtype=tf.float32)\n",
        "\n",
        "        self.tb_size = tb_size\n",
        "        self.resource_grid = resource_grid\n",
        "        self.pusch_config = pusch_config\n",
        "        \n",
        "    def update_pilots(self, pilots):\n",
        "        self.Resource_Grid_Mapper._resource_grid.pilot_pattern.pilots = pilots\n",
        "        \"\"\"Channel Estimationand Detection will reflect this update since they reference the same object.\"\"\"\n",
        "\n",
        "    def sim(self, batch_size, channel_model, no_scaling, gen_prng_seq=None, return_tx_iq=False, return_channel=False):\n",
        "        if gen_prng_seq:\n",
        "            b = tf.reshape(tf.constant(generate_prng_seq(batch_size * NUM_TX * self.tb_size, gen_prng_seq), dtype=tf.float32), [batch_size, NUM_TX, self.tb_size])\n",
        "        else:\n",
        "            b = self.Binary_Source([batch_size, NUM_TX, self.tb_size])\n",
        "\n",
        "        c = self.TB_Encoder(b)\n",
        "        x_map = self.Constellation_Mapper(c)\n",
        "        x_layer = self.Layer_Mapper(x_map)\n",
        "        x = self.Resource_Grid_Mapper(x_layer)\n",
        "\n",
        "        y, h = channel_model(x)\n",
        "        no = no_scaling * tf.math.reduce_variance(y, axis=[-1,-2,-3,-4])\n",
        "\n",
        "        y = self.AWGN([y, no])\n",
        "\n",
        "        if return_channel:\n",
        "            if return_tx_iq:\n",
        "                return b, c, y, x, h\n",
        "            return b, c, y, h\n",
        "        \n",
        "        if return_tx_iq:\n",
        "            return b, c, y, x\n",
        "        return b, c, y\n",
        "    \n",
        "    def ref(self, batch_size, dtype=tf.complex64):\n",
        "        return tf.repeat(tf.transpose(tf.constant(self.pusch_config.dmrs_grid, dtype=dtype), [0, 2, 1])[None, None], repeats=batch_size, axis=0)\n",
        "    \n",
        "    def rec(self, y, snr_ = 1e3):\n",
        "        no_ = tf.math.reduce_variance(y, axis=[-1,-2,-3,-4])/(snr_ + 1)\n",
        "        h_hat, err_var = self.Channel_Estimator([y, no_])\n",
        "        x_hat = self.Equalizer([y, h_hat, err_var, no_])\n",
        "        llr_det = self.Mimo_Detector([y, h_hat, err_var, no_])\n",
        "        llr_layer = self.Layer_Demapper(llr_det)\n",
        "        b_hat, tb_crc_status = self.TB_Decoder(llr_layer)\n",
        "\n",
        "        return h_hat, x_hat, llr_det, b_hat, tb_crc_status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "simulator = MySimulator(puschCfg)\n",
        "channel = OFDMChannel(channel_model=channel_model, resource_grid=simulator.resource_grid,\n",
        "                                    add_awgn=False, normalize_channel=True, return_channel=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "u [826608888.0] (1, 1, 4744)\n",
            "u_crc [28112077.0] (1, 1, 4768)\n",
            "u_cb [566695538.0, 782310056.0] (1, 1, 2, 2384)\n",
            "u_cb_crc [151042419.0, 911073634.0] (1, 1, 2, 2408)\n",
            "u [151042419.0, 911073634.0] (2, 2408)\n",
            "u_fill [334937524.0, 48362687.0] (2, 2560)\n",
            "s_2[:, 2*self._z:] [539127621.0, 348169825.0] (2, 2048)\n",
            "w [635687629.0, 75742688.0] (2, 10752)\n",
            "c[:, 2*self._z:] [521379575.0, 984834594.0] (2, 12800)\n",
            "c [197274699.0, 330434006.0] (2, 13312)\n",
            "c[:, 2*self._z:] [521379575.0, 984834594.0] (2, 12800)\n",
            "z 256 n 20160 n_lpdc 13312 k 2408 k_lpdc 2560\n",
            "shorten_size 20312 shorten_size-self._k_ldpc+2*self._z 18264 self._k-2*self._z 1896\n"
          ]
        },
        {
          "ename": "InvalidArgumentError",
          "evalue": "Exception encountered when calling layer 'ldpc5g_encoder_5' (type LDPC5GEncoder).\n\n{{function_node __wrapped__Slice_device_/job:localhost/replica:0/task:0/device:CPU:0}} Expected size[1] in [0, 12800], but got 20312 [Op:Slice]\n\nCall arguments received by layer 'ldpc5g_encoder_5' (type LDPC5GEncoder):\n  • inputs=tf.Tensor(shape=(1, 1, 2, 2408), dtype=float32)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mInvalidArgumentError\u001b[39m                      Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m b, c, y, x, h = \u001b[43msimulator\u001b[49m\u001b[43m.\u001b[49m\u001b[43msim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_prng_seq\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20044\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mreturn_tx_iq\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_channel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m r = simulator.ref(batch_size)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 90\u001b[39m, in \u001b[36mMySimulator.sim\u001b[39m\u001b[34m(self, batch_size, channel_model, no_scaling, gen_prng_seq, return_tx_iq, return_channel)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     88\u001b[39m     b = \u001b[38;5;28mself\u001b[39m.Binary_Source([batch_size, NUM_TX, \u001b[38;5;28mself\u001b[39m.tb_size])\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m c = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mTB_Encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m x_map = \u001b[38;5;28mself\u001b[39m.Constellation_Mapper(c)\n\u001b[32m     92\u001b[39m x_layer = \u001b[38;5;28mself\u001b[39m.Layer_Mapper(x_map)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/thanh/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     67\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 390\u001b[39m, in \u001b[36mTBEncoder.call\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    388\u001b[39m     u_cb_crc = u_cb \u001b[38;5;66;03m# no CRC applied if only one CB exists\u001b[39;00m\n\u001b[32m    389\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mu_cb_crc\u001b[39m\u001b[33m'\u001b[39m, poly_hash(u_cb_crc), u_cb_crc.shape)\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m c_cb = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu_cb_crc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mc_cb\u001b[39m\u001b[33m'\u001b[39m, poly_hash(c_cb), u_cb.shape)\n\u001b[32m    392\u001b[39m \u001b[38;5;66;03m# CB concatenation\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 762\u001b[39m, in \u001b[36mLDPC5GEncoder.call\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    760\u001b[39m shorten_size = \u001b[38;5;28mself\u001b[39m._n+\u001b[38;5;28mself\u001b[39m._k_ldpc-\u001b[38;5;28mself\u001b[39m._k\n\u001b[32m    761\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mshorten_size\u001b[39m\u001b[33m'\u001b[39m, shorten_size, \u001b[33m'\u001b[39m\u001b[33mshorten_size-self._k_ldpc+2*self._z\u001b[39m\u001b[33m'\u001b[39m, shorten_size-\u001b[38;5;28mself\u001b[39m._k_ldpc+\u001b[32m2\u001b[39m*\u001b[38;5;28mself\u001b[39m._z, \u001b[33m'\u001b[39m\u001b[33mself._k-2*self._z\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mself\u001b[39m._k-\u001b[32m2\u001b[39m*\u001b[38;5;28mself\u001b[39m._z)\n\u001b[32m--> \u001b[39m\u001b[32m762\u001b[39m c_short = \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mslice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_z\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mshorten_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    763\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mc_short\u001b[39m\u001b[33m'\u001b[39m, poly_hash(c_short), c_short.shape)\n\u001b[32m    764\u001b[39m c_no_filler1 = tf.slice(c_short, [\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m], [batch_size, \u001b[38;5;28mself\u001b[39m._k-\u001b[32m2\u001b[39m*\u001b[38;5;28mself\u001b[39m._z])\n",
            "\u001b[31mInvalidArgumentError\u001b[39m: Exception encountered when calling layer 'ldpc5g_encoder_5' (type LDPC5GEncoder).\n\n{{function_node __wrapped__Slice_device_/job:localhost/replica:0/task:0/device:CPU:0}} Expected size[1] in [0, 12800], but got 20312 [Op:Slice]\n\nCall arguments received by layer 'ldpc5g_encoder_5' (type LDPC5GEncoder):\n  • inputs=tf.Tensor(shape=(1, 1, 2, 2408), dtype=float32)"
          ]
        }
      ],
      "source": [
        "b, c, y, x, h = simulator.sim(batch_size, channel, no, gen_prng_seq=20044,return_tx_iq=True, return_channel=True)\n",
        "r = simulator.ref(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(), dtype=float64, numpy=0.003543534856812265>,\n",
              " <tf.Tensor: shape=(8, 1), dtype=bool, numpy=\n",
              " array([[ True],\n",
              "        [ True],\n",
              "        [ True],\n",
              "        [False],\n",
              "        [ True],\n",
              "        [ True],\n",
              "        [ True],\n",
              "        [ True]])>)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"Evaluate\"\"\"\n",
        "preds = predict(_model, y, r)\n",
        "c_pred = tf.reshape(preds, [preds.shape[0], 1, c.shape[-1]])\n",
        "b_hat, crc = simulator.TB_Decode(c_pred)\n",
        "# loss_cal(c_pred, c)\n",
        "compute_ber(b, b_hat), crc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(), dtype=float64, numpy=0.13842385015909747>,\n",
              " <tf.Tensor: shape=(8, 1), dtype=bool, numpy=\n",
              " array([[False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False],\n",
              "        [False]])>)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "h_est, x_hat, llr_det, b_hat, crc = simulator.rec(y)\n",
        "compute_ber(b, b_hat), crc"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
